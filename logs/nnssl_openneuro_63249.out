Node=tg11202.tamia.ecpia.ca, SLURM_NODEID=1
Node=tg11205.tamia.ecpia.ca, SLURM_NODEID=2
Node=tg11201.tamia.ecpia.ca, SLURM_NODEID=0
Using torchrun: rank=5, world_size=12, local_rank=1
Using torchrun: rank=6, world_size=12, local_rank=2
Using torchrun: rank=4, world_size=12, local_rank=0
Using torchrun: rank=7, world_size=12, local_rank=3
Using torchrun: rank=2, world_size=12, local_rank=2
Using torchrun: rank=0, world_size=12, local_rank=0Using torchrun: rank=3, world_size=12, local_rank=3Using torchrun: rank=1, world_size=12, local_rank=1


Using torchrun: rank=8, world_size=12, local_rank=0
Using torchrun: rank=9, world_size=12, local_rank=1
Using torchrun: rank=11, world_size=12, local_rank=3
Using torchrun: rank=10, world_size=12, local_rank=2
I am global rank 1, local rank 1. 4 GPUs are available. The world size is 12. Setting device to cuda:1
I am global rank 3, local rank 3. 4 GPUs are available. The world size is 12. Setting device to cuda:3
I am global rank 2, local rank 2. 4 GPUs are available. The world size is 12. Setting device to cuda:2
I am global rank 0, local rank 0. 4 GPUs are available. The world size is 12. Setting device to cuda:0
I am global rank 8, local rank 0. 4 GPUs are available. The world size is 12. Setting device to cuda:0
I am global rank 10, local rank 2. 4 GPUs are available. The world size is 12. Setting device to cuda:2
I am global rank 11, local rank 3. 4 GPUs are available. The world size is 12. Setting device to cuda:3
I am global rank 9, local rank 1. 4 GPUs are available. The world size is 12. Setting device to cuda:1
I am global rank 6, local rank 2. 4 GPUs are available. The world size is 12. Setting device to cuda:2
I am global rank 4, local rank 0. 4 GPUs are available. The world size is 12. Setting device to cuda:0
I am global rank 5, local rank 1. 4 GPUs are available. The world size is 12. Setting device to cuda:1
I am global rank 7, local rank 3. 4 GPUs are available. The world size is 12. Setting device to cuda:3

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################


#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################


#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################


This is the configuration used by this training:
Configuration name: onemmiso
 {'data_identifier': 'nnsslPlans_onemmiso', 'preprocessor_name': 'DefaultPreprocessor', 'spacing_style': 'onemmiso', 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_mask': 'resample_data_or_seg_to_shape', 'resampling_fn_mask_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'spacing': [1, 1, 1], 'patch_size': (192, 192, 64)} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset745_OpenMind', 'plans_name': 'nnsslPlans', 'original_median_spacing_after_transp': [1.1979166269302368, 1.0, 1.0], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner'} 


This is the configuration used by this training:
Configuration name: onemmiso
 {'data_identifier': 'nnsslPlans_onemmiso', 'preprocessor_name': 'DefaultPreprocessor', 'spacing_style': 'onemmiso', 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_mask': 'resample_data_or_seg_to_shape', 'resampling_fn_mask_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'spacing': [1, 1, 1], 'patch_size': (192, 192, 64)} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset745_OpenMind', 'plans_name': 'nnsslPlans', 'original_median_spacing_after_transp': [1.1979166269302368, 1.0, 1.0], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner'} 


This is the configuration used by this training:
Configuration name: onemmiso
 {'data_identifier': 'nnsslPlans_onemmiso', 'preprocessor_name': 'DefaultPreprocessor', 'spacing_style': 'onemmiso', 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_mask': 'resample_data_or_seg_to_shape', 'resampling_fn_mask_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'spacing': [1, 1, 1], 'patch_size': (192, 192, 64)} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset745_OpenMind', 'plans_name': 'nnsslPlans', 'original_median_spacing_after_transp': [1.1979166269302368, 1.0, 1.0], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner'} 

2025-08-04 11:31:12.557802: 
2025-08-04 11:31:12.559470: Epoch 0
2025-08-04 11:31:12.560726: Current learning rate: 0.0003
2025-08-04 11:31:12.570775: 
2025-08-04 11:31:12.572809: Epoch 0
2025-08-04 11:31:12.573423: Current learning rate: 0.0003
2025-08-04 11:31:12.592538: 
2025-08-04 11:31:12.592842: Epoch 0
2025-08-04 11:31:12.593376: Current learning rate: 0.0003
using pin_memory on device 2
using pin_memory on device 3
using pin_memory on device 3
using pin_memory on device 3
using pin_memory on device 0
using pin_memory on device 1
using pin_memory on device 2
using pin_memory on device 1
using pin_memory on device 1
using pin_memory on device 0
using pin_memory on device 0
using pin_memory on device 2
2025-08-04 11:39:23.680439: train_loss 4.4817
2025-08-04 11:39:23.683565: Epoch time: 491.09 s
2025-08-04 11:39:23.678905: train_loss 4.4817
2025-08-04 11:39:23.686105: train_loss 4.4817
2025-08-04 11:39:23.689308: Epoch time: 491.12 s
2025-08-04 11:39:23.688119: Epoch time: 491.11 s
2025-08-04 11:39:28.372721: 
2025-08-04 11:39:28.373128: Epoch 1
2025-08-04 11:39:28.375549: Current learning rate: 0.0003
2025-08-04 11:39:31.119687: 
2025-08-04 11:39:31.120358: Epoch 1
2025-08-04 11:39:31.120960: Current learning rate: 0.0003
2025-08-04 11:39:32.660746: 
2025-08-04 11:39:32.663302: Epoch 1
2025-08-04 11:39:32.667092: Current learning rate: 0.0003
2025-08-04 11:47:04.538294: train_loss 4.1216
2025-08-04 11:47:04.541064: train_loss 4.1216
2025-08-04 11:47:04.543429: Epoch time: 456.17 s
2025-08-04 11:47:04.542257: Epoch time: 451.88 s
2025-08-04 11:47:04.540757: train_loss 4.1216
2025-08-04 11:47:04.551520: Epoch time: 453.42 s
2025-08-04 11:47:09.598792: 
2025-08-04 11:47:09.599523: Epoch 2
2025-08-04 11:47:09.600201: Current learning rate: 0.0003
2025-08-04 11:47:11.370077: 
2025-08-04 11:47:11.372232: Epoch 2
2025-08-04 11:47:11.372952: Current learning rate: 0.0003
2025-08-04 11:47:11.473034: 
2025-08-04 11:47:11.474978: Epoch 2
2025-08-04 11:47:11.475621: Current learning rate: 0.0003
2025-08-04 11:54:39.728040: train_loss 4.0082
2025-08-04 11:54:39.731236: Epoch time: 450.13 s
2025-08-04 11:54:39.727874: train_loss 4.0082
2025-08-04 11:54:39.732322: train_loss 4.0082
2025-08-04 11:54:39.733466: Epoch time: 448.36 s
2025-08-04 11:54:39.734617: Epoch time: 448.26 s
2025-08-04 11:54:44.794816: 
2025-08-04 11:54:44.795230: Epoch 3
2025-08-04 11:54:44.795846: Current learning rate: 0.0003
2025-08-04 11:54:49.339536: 
2025-08-04 11:54:49.341824: Epoch 3
2025-08-04 11:54:49.342700: Current learning rate: 0.0003
2025-08-04 11:54:49.425400: 
2025-08-04 11:54:49.427310: Epoch 3
2025-08-04 11:54:49.428061: Current learning rate: 0.0003
2025-08-04 12:02:23.650694: train_loss 3.9386
2025-08-04 12:02:23.652561: Epoch time: 454.22 s
2025-08-04 12:02:23.656045: train_loss 3.9386
2025-08-04 12:02:23.663635: Epoch time: 458.86 s
2025-08-04 12:02:23.655707: train_loss 3.9386
2025-08-04 12:02:23.665893: Epoch time: 454.32 s
2025-08-04 12:02:29.338616: 
2025-08-04 12:02:29.342104: Epoch 4
2025-08-04 12:02:29.342849: Current learning rate: 0.0003
2025-08-04 12:02:30.892593: 
2025-08-04 12:02:30.894738: Epoch 4
2025-08-04 12:02:30.896070: Current learning rate: 0.0003
2025-08-04 12:02:31.086917: 
2025-08-04 12:02:31.087286: Epoch 4
2025-08-04 12:02:31.087858: Current learning rate: 0.0003
2025-08-04 12:09:55.999906: train_loss 3.8911
2025-08-04 12:09:56.002687: Epoch time: 444.91 s
2025-08-04 12:09:56.000090: train_loss 3.8911
2025-08-04 12:09:56.002021: train_loss 3.8911
2025-08-04 12:09:56.007254: Epoch time: 446.66 s
2025-08-04 12:09:56.008653: Epoch time: 445.11 s
2025-08-04 12:10:00.941872: 
2025-08-04 12:10:00.942266: Epoch 5
2025-08-04 12:10:00.942906: Current learning rate: 0.0003
2025-08-04 12:10:03.243759: 
2025-08-04 12:10:03.244253: Epoch 5
2025-08-04 12:10:03.245151: Current learning rate: 0.0003
2025-08-04 12:10:03.342349: 
2025-08-04 12:10:03.344811: Epoch 5
2025-08-04 12:10:03.345542: Current learning rate: 0.0003
2025-08-04 12:17:33.125628: train_loss 3.8611
2025-08-04 12:17:33.127193: Epoch time: 449.78 s
2025-08-04 12:17:33.132590: train_loss 3.8611
2025-08-04 12:17:33.134341: Epoch time: 452.19 s
2025-08-04 12:17:33.128783: train_loss 3.8611
2025-08-04 12:17:33.142193: Epoch time: 449.88 s
2025-08-04 12:17:37.378245: 
2025-08-04 12:17:37.378732: Epoch 6
2025-08-04 12:17:37.379373: Current learning rate: 0.0003
2025-08-04 12:17:38.041882: 
2025-08-04 12:17:38.044927: Epoch 6
2025-08-04 12:17:38.045581: Current learning rate: 0.0003
2025-08-04 12:17:38.746229: 
2025-08-04 12:17:38.746617: Epoch 6
2025-08-04 12:17:38.747190: Current learning rate: 0.0003
2025-08-04 12:25:16.198626: train_loss 3.8359
2025-08-04 12:25:16.197521: train_loss 3.8359
2025-08-04 12:25:16.199297: train_loss 3.8359
2025-08-04 12:25:16.201862: Epoch time: 457.45 s
2025-08-04 12:25:16.201860: Epoch time: 458.16 s
2025-08-04 12:25:16.203481: Epoch time: 458.82 s
2025-08-04 12:25:21.331371: 
2025-08-04 12:25:21.331704: Epoch 7
2025-08-04 12:25:21.332218: Current learning rate: 0.0003
2025-08-04 12:25:25.841602: 
2025-08-04 12:25:25.844287: Epoch 7
2025-08-04 12:25:25.845816: Current learning rate: 0.0003
2025-08-04 12:25:25.901965: 
2025-08-04 12:25:25.903767: Epoch 7
2025-08-04 12:25:25.904630: Current learning rate: 0.0003
2025-08-04 12:32:46.512655: train_loss 3.8194
2025-08-04 12:32:46.516011: train_loss 3.8194
2025-08-04 12:32:46.512234: train_loss 3.8194
2025-08-04 12:32:46.519063: Epoch time: 440.61 s
2025-08-04 12:32:46.517369: Epoch time: 440.68 s
2025-08-04 12:32:46.515759: Epoch time: 445.18 s
2025-08-04 12:32:51.982561: 
2025-08-04 12:32:51.982932: Epoch 8
2025-08-04 12:32:51.983526: Current learning rate: 0.0003
2025-08-04 12:32:54.780475: 
2025-08-04 12:32:54.780813: Epoch 8
2025-08-04 12:32:54.781343: Current learning rate: 0.0003
2025-08-04 12:32:54.814691: 
2025-08-04 12:32:54.816887: Epoch 8
2025-08-04 12:32:54.817596: Current learning rate: 0.0003
2025-08-04 12:40:16.445025: train_loss 3.8037
2025-08-04 12:40:16.447073: train_loss 3.8037
2025-08-04 12:40:16.447109: Epoch time: 444.46 s
2025-08-04 12:40:16.452974: Epoch time: 441.67 s
2025-08-04 12:40:16.454268: train_loss 3.8037
2025-08-04 12:40:16.460459: Epoch time: 441.64 s
2025-08-04 12:40:20.575096: 
2025-08-04 12:40:20.575503: Epoch 9
2025-08-04 12:40:20.576143: Current learning rate: 0.0003
2025-08-04 12:40:22.361882: 
2025-08-04 12:40:22.362346: Epoch 9
2025-08-04 12:40:22.363027: Current learning rate: 0.0003
2025-08-04 12:40:22.433885: 
2025-08-04 12:40:22.435991: Epoch 9
2025-08-04 12:40:22.436912: Current learning rate: 0.0003
2025-08-04 12:47:40.656329: train_loss 3.7854
2025-08-04 12:47:40.660788: train_loss 3.7854
2025-08-04 12:47:40.661511: Epoch time: 440.08 s
2025-08-04 12:47:40.661947: Epoch time: 438.23 s
2025-08-04 12:47:40.662198: train_loss 3.7854
2025-08-04 12:47:40.676058: Epoch time: 438.3 s
2025-08-04 12:47:45.581227: 
2025-08-04 12:47:45.581707: Epoch 10
2025-08-04 12:47:45.582426: Current learning rate: 0.0003
2025-08-04 12:47:48.831671: 
2025-08-04 12:47:48.834053: Epoch 10
2025-08-04 12:47:48.834699: Current learning rate: 0.0003
2025-08-04 12:47:48.905816: 
2025-08-04 12:47:48.906211: Epoch 10
2025-08-04 12:47:48.906761: Current learning rate: 0.0003
2025-08-04 12:55:07.683634: train_loss 3.7747
2025-08-04 12:55:07.684913: Epoch time: 438.85 s
2025-08-04 12:55:07.688095: train_loss 3.7747
2025-08-04 12:55:07.688328: train_loss 3.7747
2025-08-04 12:55:07.696487: Epoch time: 438.78 s
2025-08-04 12:55:07.694564: Epoch time: 442.11 s
2025-08-04 12:55:13.005319: 
2025-08-04 12:55:13.005708: Epoch 11
2025-08-04 12:55:13.006242: Current learning rate: 0.0003
2025-08-04 12:55:14.321302: 
2025-08-04 12:55:14.323676: Epoch 11
2025-08-04 12:55:14.324343: Current learning rate: 0.0003
2025-08-04 12:55:14.504228: 
2025-08-04 12:55:14.506103: Epoch 11
2025-08-04 12:55:14.506929: Current learning rate: 0.0003
2025-08-04 13:02:47.413867: train_loss 3.7651
2025-08-04 13:02:47.419037: train_loss 3.7651
2025-08-04 13:02:47.420284: Epoch time: 454.41 s
2025-08-04 13:02:47.416952: train_loss 3.7651
2025-08-04 13:02:47.419245: Epoch time: 453.09 s
2025-08-04 13:02:47.424463: Epoch time: 452.91 s
2025-08-04 13:02:52.662333: 
2025-08-04 13:02:52.662731: Epoch 12
2025-08-04 13:02:52.663293: Current learning rate: 0.0003
2025-08-04 13:02:55.843493: 
2025-08-04 13:02:55.843939: Epoch 12
2025-08-04 13:02:55.844815: Current learning rate: 0.0003
2025-08-04 13:02:55.873394: 
2025-08-04 13:02:55.876295: Epoch 12
2025-08-04 13:02:55.877044: Current learning rate: 0.0003
2025-08-04 13:10:18.057440: train_loss 3.7604
2025-08-04 13:10:18.058623: Epoch time: 445.4 s
2025-08-04 13:10:18.058540: train_loss 3.7604
2025-08-04 13:10:18.058214: train_loss 3.7604
2025-08-04 13:10:18.061466: Epoch time: 442.21 s
2025-08-04 13:10:18.059922: Epoch time: 442.19 s
2025-08-04 13:10:23.065575: 
2025-08-04 13:10:23.065962: Epoch 13
2025-08-04 13:10:23.066506: Current learning rate: 0.0003
2025-08-04 13:10:24.489216: 
2025-08-04 13:10:24.491537: Epoch 13
2025-08-04 13:10:24.492688: Current learning rate: 0.0003
2025-08-04 13:10:24.599538: 
2025-08-04 13:10:24.601752: Epoch 13
2025-08-04 13:10:24.602377: Current learning rate: 0.0003
2025-08-04 13:17:46.473194: train_loss 3.7501
2025-08-04 13:17:46.473469: train_loss 3.7501
2025-08-04 13:17:46.476220: Epoch time: 441.87 s
2025-08-04 13:17:46.477129: Epoch time: 443.41 s
2025-08-04 13:17:46.480350: train_loss 3.7501
2025-08-04 13:17:46.484867: Epoch time: 441.99 s
2025-08-04 13:17:51.153041: 
2025-08-04 13:17:51.153428: Epoch 14
2025-08-04 13:17:51.153981: Current learning rate: 0.0003
2025-08-04 13:17:51.659968: 
2025-08-04 13:17:51.663145: Epoch 14
2025-08-04 13:17:51.663773: Current learning rate: 0.0003
2025-08-04 13:17:52.261392: 
2025-08-04 13:17:52.263823: Epoch 14
2025-08-04 13:17:52.264573: Current learning rate: 0.0003
2025-08-04 13:25:20.005580: train_loss 3.7446
2025-08-04 13:25:20.007168: Epoch time: 448.35 s
2025-08-04 13:25:20.012030: train_loss 3.7446
2025-08-04 13:25:20.016633: Epoch time: 448.85 s
2025-08-04 13:25:20.012985: train_loss 3.7446
2025-08-04 13:25:20.021690: Epoch time: 447.74 s
2025-08-04 13:25:26.065783: 
2025-08-04 13:25:26.069002: Epoch 15
2025-08-04 13:25:26.070433: Current learning rate: 0.0003
2025-08-04 13:25:29.297784: 
2025-08-04 13:25:29.298135: Epoch 15
2025-08-04 13:25:29.298695: Current learning rate: 0.0003
2025-08-04 13:25:29.438778: 
2025-08-04 13:25:29.441337: Epoch 15
2025-08-04 13:25:29.443660: Current learning rate: 0.0003
2025-08-04 13:32:44.985034: train_loss 3.7381
2025-08-04 13:32:44.986332: Epoch time: 438.92 s
2025-08-04 13:32:44.989628: train_loss 3.7381
2025-08-04 13:32:44.992976: Epoch time: 435.69 s
2025-08-04 13:32:44.984072: train_loss 3.7381
2025-08-04 13:32:45.000488: Epoch time: 435.55 s
2025-08-04 13:32:50.430732: 
2025-08-04 13:32:50.431096: Epoch 16
2025-08-04 13:32:50.431673: Current learning rate: 0.0003
2025-08-04 13:32:55.072663: 
2025-08-04 13:32:55.075366: Epoch 16
2025-08-04 13:32:55.075984: Current learning rate: 0.0003
2025-08-04 13:32:55.094048: 
2025-08-04 13:32:55.096304: Epoch 16
2025-08-04 13:32:55.097091: Current learning rate: 0.0003
2025-08-04 13:40:21.371848: train_loss 3.7369
2025-08-04 13:40:21.375770: Epoch time: 450.94 s
2025-08-04 13:40:21.376198: train_loss 3.7369
2025-08-04 13:40:21.375038: train_loss 3.7369
2025-08-04 13:40:21.381461: Epoch time: 446.28 s
2025-08-04 13:40:21.379773: Epoch time: 446.3 s
2025-08-04 13:40:25.657363: 
2025-08-04 13:40:25.660091: Epoch 17
2025-08-04 13:40:25.667526: Current learning rate: 0.0003
2025-08-04 13:40:30.274664: 
2025-08-04 13:40:30.277804: Epoch 17
2025-08-04 13:40:30.278642: Current learning rate: 0.0003
2025-08-04 13:40:30.292640: 
2025-08-04 13:40:30.292984: Epoch 17
2025-08-04 13:40:30.293510: Current learning rate: 0.0003
2025-08-04 13:47:56.743465: train_loss 3.7309
2025-08-04 13:47:56.749148: Epoch time: 446.45 s
2025-08-04 13:47:56.746133: train_loss 3.7309
2025-08-04 13:47:56.750178: Epoch time: 446.47 s
2025-08-04 13:47:56.750832: train_loss 3.7309
2025-08-04 13:47:56.757353: Epoch time: 451.09 s
2025-08-04 13:48:01.773096: 
2025-08-04 13:48:01.775581: Epoch 18
2025-08-04 13:48:01.776226: Current learning rate: 0.0003
2025-08-04 13:48:02.235521: 
2025-08-04 13:48:02.238055: Epoch 18
2025-08-04 13:48:02.238745: Current learning rate: 0.0003
2025-08-04 13:48:09.129109: 
2025-08-04 13:48:09.130211: Epoch 18
2025-08-04 13:48:09.130843: Current learning rate: 0.0003
2025-08-04 13:55:19.538787: train_loss 3.7247
2025-08-04 13:55:19.541219: Epoch time: 430.41 s
2025-08-04 13:55:19.541263: train_loss 3.7247
2025-08-04 13:55:19.548718: Epoch time: 437.31 s
2025-08-04 13:55:19.545813: train_loss 3.7247
2025-08-04 13:55:19.552304: Epoch time: 437.77 s
2025-08-04 13:55:24.335806: 
2025-08-04 13:55:24.336777: Epoch 19
2025-08-04 13:55:24.337401: Current learning rate: 0.0003
2025-08-04 13:55:27.204945: 
2025-08-04 13:55:27.205292: Epoch 19
2025-08-04 13:55:27.205828: Current learning rate: 0.0003
2025-08-04 13:55:27.275180: 
2025-08-04 13:55:27.278098: Epoch 19
2025-08-04 13:55:27.279412: Current learning rate: 0.0003
2025-08-04 14:02:49.205892: train_loss 3.7216
2025-08-04 14:02:49.207080: Epoch time: 441.93 s
2025-08-04 14:02:49.211970: train_loss 3.7216
2025-08-04 14:02:49.209104: train_loss 3.7216
2025-08-04 14:02:49.219333: Epoch time: 442.01 s
2025-08-04 14:02:49.217903: Epoch time: 444.88 s
2025-08-04 14:02:52.991799: 
2025-08-04 14:02:52.994567: Epoch 20
2025-08-04 14:02:53.002153: Current learning rate: 0.0003
2025-08-04 14:02:54.777837: 
2025-08-04 14:02:54.778204: Epoch 20
2025-08-04 14:02:54.778756: Current learning rate: 0.0003
2025-08-04 14:02:54.899558: 
2025-08-04 14:02:54.902380: Epoch 20
2025-08-04 14:02:54.903200: Current learning rate: 0.0003
2025-08-04 14:10:18.370179: train_loss 3.7196
2025-08-04 14:10:18.372734: train_loss 3.7196
2025-08-04 14:10:18.373376: train_loss 3.7196
2025-08-04 14:10:18.374012: Epoch time: 445.38 s
2025-08-04 14:10:18.376631: Epoch time: 443.47 s
2025-08-04 14:10:18.377281: Epoch time: 443.59 s
2025-08-04 14:10:23.757602: 
2025-08-04 14:10:23.760821: Epoch 21
2025-08-04 14:10:23.761697: Current learning rate: 0.0003
2025-08-04 14:10:24.234108: 
2025-08-04 14:10:24.234464: Epoch 21
2025-08-04 14:10:24.235011: Current learning rate: 0.0003
2025-08-04 14:10:24.865430: 
2025-08-04 14:10:24.867835: Epoch 21
2025-08-04 14:10:24.868452: Current learning rate: 0.0003
2025-08-04 14:17:43.861827: train_loss 3.7148
2025-08-04 14:17:43.865799: train_loss 3.7148
2025-08-04 14:17:43.868730: Epoch time: 439.63 s
2025-08-04 14:17:43.867884: Epoch time: 440.11 s
2025-08-04 14:17:43.880693: train_loss 3.7148
2025-08-04 14:17:43.883741: Epoch time: 439.02 s
2025-08-04 14:17:48.766271: 
2025-08-04 14:17:48.768520: Epoch 22
2025-08-04 14:17:48.769204: Current learning rate: 0.0003
2025-08-04 14:17:50.964639: 
2025-08-04 14:17:50.966578: Epoch 22
2025-08-04 14:17:50.968386: Current learning rate: 0.0003
2025-08-04 14:17:51.103385: 
2025-08-04 14:17:51.103791: Epoch 22
2025-08-04 14:17:51.104450: Current learning rate: 0.0003
2025-08-04 14:25:19.747726: train_loss 3.7137
2025-08-04 14:25:19.748847: train_loss 3.7137
2025-08-04 14:25:19.752353: Epoch time: 450.98 s
2025-08-04 14:25:19.743874: train_loss 3.7137
2025-08-04 14:25:19.755515: Epoch time: 448.64 s
2025-08-04 14:25:19.750548: Epoch time: 448.78 s
2025-08-04 14:25:24.318926: 
2025-08-04 14:25:24.320110: Epoch 23
2025-08-04 14:25:24.325239: Current learning rate: 0.0003
2025-08-04 14:25:27.307956: 
2025-08-04 14:25:27.308293: Epoch 23
2025-08-04 14:25:27.308830: Current learning rate: 0.0003
2025-08-04 14:25:27.339742: 
2025-08-04 14:25:27.341883: Epoch 23
2025-08-04 14:25:27.342950: Current learning rate: 0.0003
2025-08-04 14:32:54.068189: train_loss 3.7106
2025-08-04 14:32:54.073456: Epoch time: 449.75 s
2025-08-04 14:32:54.074896: train_loss 3.7106
2025-08-04 14:32:54.074469: train_loss 3.7106
2025-08-04 14:32:54.081112: Epoch time: 446.77 s
2025-08-04 14:32:54.082033: Epoch time: 446.74 s
2025-08-04 14:32:58.938122: 
2025-08-04 14:32:58.941405: Epoch 24
2025-08-04 14:32:58.942019: Current learning rate: 0.0003
2025-08-04 14:33:01.641910: 
2025-08-04 14:33:01.644651: Epoch 24
2025-08-04 14:33:01.645362: Current learning rate: 0.0003
2025-08-04 14:33:01.758934: 
2025-08-04 14:33:01.759331: Epoch 24
2025-08-04 14:33:01.759898: Current learning rate: 0.0003
2025-08-04 14:40:23.908256: train_loss 3.707
2025-08-04 14:40:23.905677: train_loss 3.707
2025-08-04 14:40:23.911249: Epoch time: 442.15 s
2025-08-04 14:40:23.912408: Epoch time: 444.97 s
2025-08-04 14:40:23.913301: train_loss 3.707
2025-08-04 14:40:23.919414: Epoch time: 442.27 s
2025-08-04 14:40:27.787236: Saving checkpoint at epoch 25...
2025-08-04 14:40:28.813405: Saving checkpoint at epoch 25...
2025-08-04 14:40:29.022779: Saving checkpoint at epoch 25...
2025-08-04 14:40:32.020830: 
2025-08-04 14:40:32.021181: Epoch 25
2025-08-04 14:40:32.021742: Current learning rate: 0.0003
2025-08-04 14:40:33.669538: 
2025-08-04 14:40:33.696435: Epoch 25
2025-08-04 14:40:33.697303: Current learning rate: 0.0003
2025-08-04 14:40:33.869563: 
2025-08-04 14:40:33.876275: Epoch 25
2025-08-04 14:40:33.876963: Current learning rate: 0.0003
2025-08-04 14:47:57.199144: train_loss 3.7068
2025-08-04 14:47:57.197723: train_loss 3.7068
2025-08-04 14:47:57.199980: train_loss 3.7068
2025-08-04 14:47:57.201226: Epoch time: 443.53 s
2025-08-04 14:47:57.206666: Epoch time: 445.18 s
2025-08-04 14:47:57.202589: Epoch time: 443.33 s
2025-08-04 14:48:02.285031: 
2025-08-04 14:48:02.286908: Epoch 26
2025-08-04 14:48:02.288335: Current learning rate: 0.0003
2025-08-04 14:48:05.817466: 
2025-08-04 14:48:05.819678: Epoch 26
2025-08-04 14:48:05.820393: Current learning rate: 0.0003
2025-08-04 14:48:05.967567: 
2025-08-04 14:48:05.967909: Epoch 26
2025-08-04 14:48:05.968428: Current learning rate: 0.0003
2025-08-04 14:55:20.086922: train_loss 3.7011
2025-08-04 14:55:20.092432: Epoch time: 434.27 s
2025-08-04 14:55:20.091630: train_loss 3.7011
2025-08-04 14:55:20.097391: Epoch time: 434.12 s
2025-08-04 14:55:20.086973: train_loss 3.7011
2025-08-04 14:55:20.099832: Epoch time: 437.8 s
2025-08-04 14:55:24.019043: 
2025-08-04 14:55:24.019452: Epoch 27
2025-08-04 14:55:24.020035: Current learning rate: 0.0003
2025-08-04 14:55:25.660885: 
2025-08-04 14:55:25.661236: Epoch 27
2025-08-04 14:55:25.661768: Current learning rate: 0.0003
2025-08-04 14:55:25.753867: 
2025-08-04 14:55:25.756110: Epoch 27
2025-08-04 14:55:25.756832: Current learning rate: 0.0003
2025-08-04 15:02:52.774040: train_loss 3.7
2025-08-04 15:02:52.773018: train_loss 3.7
2025-08-04 15:02:52.780653: Epoch time: 447.11 s
2025-08-04 15:02:52.775450: Epoch time: 447.02 s
2025-08-04 15:02:52.773998: train_loss 3.7
2025-08-04 15:02:52.792958: Epoch time: 448.75 s
2025-08-04 15:02:57.931489: 
2025-08-04 15:02:57.932368: Epoch 28
2025-08-04 15:02:57.932942: Current learning rate: 0.0003
2025-08-04 15:03:02.403184: 
2025-08-04 15:03:02.403544: Epoch 28
2025-08-04 15:03:02.404102: Current learning rate: 0.0003
2025-08-04 15:03:02.406013: 
2025-08-04 15:03:02.408447: Epoch 28
2025-08-04 15:03:02.410791: Current learning rate: 0.0003
2025-08-04 15:10:28.028107: train_loss 3.6967
2025-08-04 15:10:28.037005: train_loss 3.6967
2025-08-04 15:10:28.029824: Epoch time: 450.1 s
2025-08-04 15:10:28.028904: train_loss 3.6967
2025-08-04 15:10:28.040162: Epoch time: 445.62 s
2025-08-04 15:10:28.040820: Epoch time: 445.63 s
2025-08-04 15:10:32.545200: 
2025-08-04 15:10:32.552501: Epoch 29
2025-08-04 15:10:32.559851: Current learning rate: 0.0003
2025-08-04 15:10:36.050762: 
2025-08-04 15:10:36.051104: Epoch 29
2025-08-04 15:10:36.051716: Current learning rate: 0.0003
2025-08-04 15:10:37.377663: 
2025-08-04 15:10:37.380230: Epoch 29
2025-08-04 15:10:37.381372: Current learning rate: 0.0003
2025-08-04 15:17:54.018762: train_loss 3.6942
2025-08-04 15:17:54.020111: Epoch time: 436.64 s
2025-08-04 15:17:54.016571: train_loss 3.6942
2025-08-04 15:17:54.018056: train_loss 3.6942
2025-08-04 15:17:54.022577: Epoch time: 441.47 s
2025-08-04 15:17:54.022614: Epoch time: 437.97 s
2025-08-04 15:18:01.560599: 
2025-08-04 15:18:01.561046: Epoch 30
2025-08-04 15:18:01.561755: Current learning rate: 0.0003
2025-08-04 15:18:03.103816: 
2025-08-04 15:18:03.106721: Epoch 30
2025-08-04 15:18:03.107817: Current learning rate: 0.0003
2025-08-04 15:18:03.227636: 
2025-08-04 15:18:03.227988: Epoch 30
2025-08-04 15:18:03.228559: Current learning rate: 0.0003
2025-08-04 15:25:24.786203: train_loss 3.6958
2025-08-04 15:25:24.788674: train_loss 3.6958
2025-08-04 15:25:24.786032: train_loss 3.6958
2025-08-04 15:25:24.790388: Epoch time: 441.56 s
2025-08-04 15:25:24.789925: Epoch time: 443.23 s
2025-08-04 15:25:24.791788: Epoch time: 441.68 s
2025-08-04 15:25:29.938519: 
2025-08-04 15:25:29.938936: Epoch 31
2025-08-04 15:25:29.945900: Current learning rate: 0.0003
2025-08-04 15:25:31.803810: 
2025-08-04 15:25:31.806556: Epoch 31
2025-08-04 15:25:31.808614: Current learning rate: 0.0003
2025-08-04 15:25:31.981355: 
2025-08-04 15:25:31.981730: Epoch 31
2025-08-04 15:25:31.982271: Current learning rate: 0.0003
2025-08-04 15:32:55.244028: train_loss 3.6909
2025-08-04 15:32:55.246147: Epoch time: 443.26 s
2025-08-04 15:32:55.238983: train_loss 3.6909
2025-08-04 15:32:55.245206: train_loss 3.6909
2025-08-04 15:32:55.248850: Epoch time: 445.3 s
2025-08-04 15:32:55.250220: Epoch time: 443.44 s
2025-08-04 15:33:00.037503: 
2025-08-04 15:33:00.039765: Epoch 32
2025-08-04 15:33:00.040583: Current learning rate: 0.0003
2025-08-04 15:33:00.389164: 
2025-08-04 15:33:00.391005: Epoch 32
2025-08-04 15:33:00.391896: Current learning rate: 0.0003
2025-08-04 15:33:06.731285: 
2025-08-04 15:33:06.732284: Epoch 32
2025-08-04 15:33:06.732876: Current learning rate: 0.0003
2025-08-04 15:40:20.676704: train_loss 3.6905
2025-08-04 15:40:20.680409: Epoch time: 440.64 s
2025-08-04 15:40:20.675249: train_loss 3.6905
2025-08-04 15:40:20.683237: Epoch time: 440.29 s
2025-08-04 15:40:20.680814: train_loss 3.6905
2025-08-04 15:40:20.686008: Epoch time: 433.95 s
2025-08-04 15:40:25.011288: 
2025-08-04 15:40:25.012964: Epoch 33
2025-08-04 15:40:25.013665: Current learning rate: 0.0003
2025-08-04 15:40:26.463191: 
2025-08-04 15:40:26.465343: Epoch 33
2025-08-04 15:40:26.466406: Current learning rate: 0.0003
2025-08-04 15:40:26.608693: 
2025-08-04 15:40:26.609037: Epoch 33
2025-08-04 15:40:26.609566: Current learning rate: 0.0003
2025-08-04 15:47:57.874762: train_loss 3.6873
2025-08-04 15:47:57.876198: Epoch time: 452.86 s
2025-08-04 15:47:57.877296: train_loss 3.6873
2025-08-04 15:47:57.880655: Epoch time: 451.27 s
2025-08-04 15:47:57.874085: train_loss 3.6873
2025-08-04 15:47:57.882833: Epoch time: 451.41 s
2025-08-04 15:48:02.969736: 
2025-08-04 15:48:02.970107: Epoch 34
2025-08-04 15:48:02.970710: Current learning rate: 0.0003
2025-08-04 15:48:05.424871: 
2025-08-04 15:48:05.425328: Epoch 34
2025-08-04 15:48:05.426008: Current learning rate: 0.0003
2025-08-04 15:48:05.527549: 
2025-08-04 15:48:05.529685: Epoch 34
2025-08-04 15:48:05.530332: Current learning rate: 0.0003
2025-08-04 15:55:26.932478: train_loss 3.6883
2025-08-04 15:55:26.933179: train_loss 3.6883
2025-08-04 15:55:26.936958: Epoch time: 441.51 s
2025-08-04 15:55:26.934453: Epoch time: 441.41 s
2025-08-04 15:55:26.936655: train_loss 3.6883
2025-08-04 15:55:26.939619: Epoch time: 443.97 s
2025-08-04 15:55:31.200380: 
2025-08-04 15:55:31.201328: Epoch 35
2025-08-04 15:55:31.202056: Current learning rate: 0.0003
2025-08-04 15:55:31.820016: 
2025-08-04 15:55:31.822343: Epoch 35
2025-08-04 15:55:31.823061: Current learning rate: 0.0003
2025-08-04 15:55:33.330070: 
2025-08-04 15:55:33.330524: Epoch 35
2025-08-04 15:55:33.331162: Current learning rate: 0.0003
2025-08-04 16:02:47.354740: train_loss 3.6852
2025-08-04 16:02:47.356519: Epoch time: 436.15 s
2025-08-04 16:02:47.358053: train_loss 3.6852
2025-08-04 16:02:47.357196: train_loss 3.6852
2025-08-04 16:02:47.368472: Epoch time: 435.54 s
2025-08-04 16:02:47.369075: Epoch time: 434.02 s
2025-08-04 16:02:52.755489: 
2025-08-04 16:02:52.758603: Epoch 36
2025-08-04 16:02:52.766198: Current learning rate: 0.0003
2025-08-04 16:02:54.158707: 
2025-08-04 16:02:54.159052: Epoch 36
2025-08-04 16:02:54.159572: Current learning rate: 0.0003
2025-08-04 16:02:54.285046: 
2025-08-04 16:02:54.287244: Epoch 36
2025-08-04 16:02:54.288901: Current learning rate: 0.0003
2025-08-04 16:10:08.890969: train_loss 3.6861
2025-08-04 16:10:08.891544: train_loss 3.6861
2025-08-04 16:10:08.892495: Epoch time: 434.6 s
2025-08-04 16:10:08.894256: Epoch time: 436.14 s
2025-08-04 16:10:08.892038: train_loss 3.6861
2025-08-04 16:10:08.901713: Epoch time: 434.73 s
2025-08-04 16:10:13.974091: 
2025-08-04 16:10:13.974432: Epoch 37
2025-08-04 16:10:13.974967: Current learning rate: 0.0003
2025-08-04 16:10:16.686372: 
2025-08-04 16:10:16.686938: Epoch 37
2025-08-04 16:10:16.687548: Current learning rate: 0.0003
2025-08-04 16:10:16.698806: 
2025-08-04 16:10:16.700943: Epoch 37
2025-08-04 16:10:16.701588: Current learning rate: 0.0003
2025-08-04 16:17:44.415906: train_loss 3.6828
2025-08-04 16:17:44.417244: Epoch time: 447.72 s
2025-08-04 16:17:44.415166: train_loss 3.6828
2025-08-04 16:17:44.420676: Epoch time: 450.44 s
2025-08-04 16:17:44.416313: train_loss 3.6828
2025-08-04 16:17:44.423365: Epoch time: 447.73 s
2025-08-04 16:17:48.821232: 
2025-08-04 16:17:48.822314: Epoch 38
2025-08-04 16:17:48.822857: Current learning rate: 0.0003
2025-08-04 16:17:50.071736: 
2025-08-04 16:17:50.073925: Epoch 38
2025-08-04 16:17:50.074632: Current learning rate: 0.0003
2025-08-04 16:17:50.471071: 
2025-08-04 16:17:50.471449: Epoch 38
2025-08-04 16:17:50.472028: Current learning rate: 0.0003
2025-08-04 16:25:10.061451: train_loss 3.6798
2025-08-04 16:25:10.065964: Epoch time: 439.99 s
2025-08-04 16:25:10.067370: train_loss 3.6798
2025-08-04 16:25:10.069220: Epoch time: 439.6 s
2025-08-04 16:25:10.071833: train_loss 3.6798
2025-08-04 16:25:10.080258: Epoch time: 441.25 s
2025-08-04 16:25:15.686653: 
2025-08-04 16:25:15.687024: Epoch 39
2025-08-04 16:25:15.687595: Current learning rate: 0.0003
2025-08-04 16:25:20.491484: 
2025-08-04 16:25:20.493276: Epoch 39
2025-08-04 16:25:20.493942: Current learning rate: 0.0003
2025-08-04 16:25:20.602256: 
2025-08-04 16:25:20.604035: Epoch 39
2025-08-04 16:25:20.604707: Current learning rate: 0.0003
2025-08-04 16:32:45.415378: train_loss 3.6793
2025-08-04 16:32:45.413537: train_loss 3.6793
2025-08-04 16:32:45.418913: Epoch time: 449.73 s
2025-08-04 16:32:45.421891: train_loss 3.6793
2025-08-04 16:32:45.424386: Epoch time: 444.82 s
2025-08-04 16:32:45.418438: Epoch time: 444.92 s
2025-08-04 16:32:50.366792: 
2025-08-04 16:32:50.367237: Epoch 40
2025-08-04 16:32:50.367882: Current learning rate: 0.0003
2025-08-04 16:32:51.788226: 
2025-08-04 16:32:51.791232: Epoch 40
2025-08-04 16:32:51.791804: Current learning rate: 0.0003
2025-08-04 16:32:51.896769: 
2025-08-04 16:32:51.898592: Epoch 40
2025-08-04 16:32:51.899364: Current learning rate: 0.0003
2025-08-04 16:40:17.353618: train_loss 3.6789
2025-08-04 16:40:17.354961: Epoch time: 445.57 s
2025-08-04 16:40:17.357549: train_loss 3.6789
2025-08-04 16:40:17.359034: Epoch time: 446.99 s
2025-08-04 16:40:17.356986: train_loss 3.6789
2025-08-04 16:40:17.364711: Epoch time: 445.46 s
2025-08-04 16:40:22.661943: 
2025-08-04 16:40:22.662376: Epoch 41
2025-08-04 16:40:22.663025: Current learning rate: 0.0003
2025-08-04 16:40:24.203631: 
2025-08-04 16:40:24.206604: Epoch 41
2025-08-04 16:40:24.207517: Current learning rate: 0.0003
2025-08-04 16:40:24.417422: 
2025-08-04 16:40:24.417784: Epoch 41
2025-08-04 16:40:24.418337: Current learning rate: 0.0003
2025-08-04 16:47:46.174505: train_loss 3.6746
2025-08-04 16:47:46.176866: train_loss 3.6746
2025-08-04 16:47:46.178079: Epoch time: 441.76 s
2025-08-04 16:47:46.177879: Epoch time: 443.51 s
2025-08-04 16:47:46.176709: train_loss 3.6746
2025-08-04 16:47:46.186534: Epoch time: 441.97 s
2025-08-04 16:47:51.373569: 
2025-08-04 16:47:51.374614: Epoch 42
2025-08-04 16:47:51.375237: Current learning rate: 0.0003
2025-08-04 16:47:52.497416: 
2025-08-04 16:47:52.497829: Epoch 42
2025-08-04 16:47:52.498400: Current learning rate: 0.0003
2025-08-04 16:47:52.657672: 
2025-08-04 16:47:52.659635: Epoch 42
2025-08-04 16:47:52.661706: Current learning rate: 0.0003
2025-08-04 16:55:21.873576: train_loss 3.671
2025-08-04 16:55:21.880438: train_loss 3.671
2025-08-04 16:55:21.882823: Epoch time: 450.51 s
2025-08-04 16:55:21.880735: Epoch time: 449.21 s
2025-08-04 16:55:21.882068: train_loss 3.671
2025-08-04 16:55:21.884411: Epoch time: 449.38 s
2025-08-04 16:55:27.797477: 
2025-08-04 16:55:27.797923: Epoch 43
2025-08-04 16:55:27.798627: Current learning rate: 0.0003
2025-08-04 16:55:31.636811: 
2025-08-04 16:55:31.638977: Epoch 43
2025-08-04 16:55:31.639826: Current learning rate: 0.0003
2025-08-04 16:55:31.642214: 
2025-08-04 16:55:31.642596: Epoch 43
2025-08-04 16:55:31.643242: Current learning rate: 0.0003
2025-08-04 17:02:57.642599: train_loss 3.6721
2025-08-04 17:02:57.647315: train_loss 3.6721
2025-08-04 17:02:57.648388: Epoch time: 446.01 s
2025-08-04 17:02:57.647848: Epoch time: 446.01 s
2025-08-04 17:02:57.649570: train_loss 3.6721
2025-08-04 17:02:57.655987: Epoch time: 449.85 s
2025-08-04 17:03:03.867387: 
2025-08-04 17:03:03.867743: Epoch 44
2025-08-04 17:03:03.868318: Current learning rate: 0.0003
2025-08-04 17:03:06.024225: 
2025-08-04 17:03:06.026747: Epoch 44
2025-08-04 17:03:06.027555: Current learning rate: 0.0003
2025-08-04 17:03:06.169254: 
2025-08-04 17:03:06.171268: Epoch 44
2025-08-04 17:03:06.171991: Current learning rate: 0.0003
2025-08-04 17:10:32.345317: train_loss 3.6688
2025-08-04 17:10:32.340004: train_loss 3.6688
2025-08-04 17:10:32.348661: Epoch time: 448.47 s
2025-08-04 17:10:32.344620: train_loss 3.6688
2025-08-04 17:10:32.349680: Epoch time: 446.18 s
2025-08-04 17:10:32.347435: Epoch time: 446.32 s
2025-08-04 17:10:36.219531: 
2025-08-04 17:10:36.219944: Epoch 45
2025-08-04 17:10:36.220954: Current learning rate: 0.0003
2025-08-04 17:10:38.287117: 
2025-08-04 17:10:38.287492: Epoch 45
2025-08-04 17:10:38.288046: Current learning rate: 0.0003
2025-08-04 17:10:38.668844: 
2025-08-04 17:10:38.670924: Epoch 45
2025-08-04 17:10:38.671776: Current learning rate: 0.0003
2025-08-04 17:18:01.738642: train_loss 3.6697
2025-08-04 17:18:01.741245: train_loss 3.6697
2025-08-04 17:18:01.743862: Epoch time: 443.45 s
2025-08-04 17:18:01.746967: Epoch time: 445.52 s
2025-08-04 17:18:01.742167: train_loss 3.6697
2025-08-04 17:18:01.751108: Epoch time: 443.07 s
2025-08-04 17:18:06.047511: 
2025-08-04 17:18:06.050229: Epoch 46
2025-08-04 17:18:06.051476: Current learning rate: 0.0003
2025-08-04 17:18:09.533787: 
2025-08-04 17:18:09.534135: Epoch 46
2025-08-04 17:18:09.534725: Current learning rate: 0.0003
2025-08-04 17:18:15.948484: 
2025-08-04 17:18:15.950942: Epoch 46
2025-08-04 17:18:15.961451: Current learning rate: 0.0003
2025-08-04 17:25:37.548982: train_loss 3.6672
2025-08-04 17:25:37.549907: train_loss 3.6672
2025-08-04 17:25:37.550217: Epoch time: 441.6 s
2025-08-04 17:25:37.542959: train_loss 3.6672
2025-08-04 17:25:37.552133: Epoch time: 451.5 s
2025-08-04 17:25:37.553153: Epoch time: 448.01 s
2025-08-04 17:25:43.000345: 
2025-08-04 17:25:43.001731: Epoch 47
2025-08-04 17:25:43.002358: Current learning rate: 0.0003
2025-08-04 17:25:45.269842: 
2025-08-04 17:25:45.272337: Epoch 47
2025-08-04 17:25:45.273889: Current learning rate: 0.0003
2025-08-04 17:25:45.423118: 
2025-08-04 17:25:45.423461: Epoch 47
2025-08-04 17:25:45.424032: Current learning rate: 0.0003
2025-08-04 17:33:10.190742: train_loss 3.6676
2025-08-04 17:33:10.190964: train_loss 3.6676
2025-08-04 17:33:10.192127: Epoch time: 444.92 s
2025-08-04 17:33:10.193475: Epoch time: 447.19 s
2025-08-04 17:33:10.193817: train_loss 3.6676
2025-08-04 17:33:10.197848: Epoch time: 444.77 s
2025-08-04 17:33:14.471588: 
2025-08-04 17:33:14.471980: Epoch 48
2025-08-04 17:33:14.472633: Current learning rate: 0.0003
2025-08-04 17:33:16.852242: 
2025-08-04 17:33:16.852602: Epoch 48
2025-08-04 17:33:16.853154: Current learning rate: 0.0003
2025-08-04 17:33:16.991409: 
2025-08-04 17:33:16.993590: Epoch 48
2025-08-04 17:33:16.994302: Current learning rate: 0.0003
2025-08-04 17:40:32.239829: train_loss 3.6645
2025-08-04 17:40:32.242070: train_loss 3.6645
2025-08-04 17:40:32.244959: Epoch time: 435.39 s
2025-08-04 17:40:32.242701: Epoch time: 435.25 s
2025-08-04 17:40:32.251951: train_loss 3.6645
2025-08-04 17:40:32.257575: Epoch time: 437.78 s
2025-08-04 17:40:37.586238: 
2025-08-04 17:40:37.589033: Epoch 49
2025-08-04 17:40:37.589762: Current learning rate: 0.0003
2025-08-04 17:40:37.971801: 
2025-08-04 17:40:37.974119: Epoch 49
2025-08-04 17:40:37.974738: Current learning rate: 0.0003
2025-08-04 17:40:38.600854: 
2025-08-04 17:40:38.601226: Epoch 49
2025-08-04 17:40:38.601778: Current learning rate: 0.0003
2025-08-04 17:48:00.671581: train_loss 3.6641
2025-08-04 17:48:00.671891: train_loss 3.6641
2025-08-04 17:48:00.678139: Epoch time: 442.07 s
2025-08-04 17:48:00.675039: train_loss 3.6641
2025-08-04 17:48:00.681276: Epoch time: 442.7 s
2025-08-04 17:48:00.679234: Epoch time: 443.09 s
2025-08-04 17:48:04.775767: Saving checkpoint at epoch 50...
2025-08-04 17:48:05.556737: Saving checkpoint at epoch 50...
2025-08-04 17:48:08.037433: 
2025-08-04 17:48:08.037816: Epoch 50
2025-08-04 17:48:08.038421: Current learning rate: 0.0003
2025-08-04 17:48:11.196172: Saving checkpoint at epoch 50...
2025-08-04 17:48:15.416852: 
2025-08-04 17:48:15.418120: Epoch 50
2025-08-04 17:48:15.418890: Current learning rate: 0.0003
2025-08-04 17:48:17.111465: 
2025-08-04 17:48:17.111896: Epoch 50
2025-08-04 17:48:17.122845: Current learning rate: 0.0003
2025-08-04 17:55:34.432871: train_loss 3.6629
2025-08-04 17:55:34.436090: Epoch time: 439.02 s
2025-08-04 17:55:34.434211: train_loss 3.6629
2025-08-04 17:55:34.438982: Epoch time: 446.4 s
2025-08-04 17:55:34.435010: train_loss 3.6629
2025-08-04 17:55:34.450228: Epoch time: 437.32 s
2025-08-04 17:55:39.985353: 
2025-08-04 17:55:39.988361: Epoch 51
2025-08-04 17:55:39.989046: Current learning rate: 0.0003
2025-08-04 17:55:43.589077: 
2025-08-04 17:55:43.591787: Epoch 51
2025-08-04 17:55:43.593914: Current learning rate: 0.0003
2025-08-04 17:55:43.731010: 
2025-08-04 17:55:43.731357: Epoch 51
2025-08-04 17:55:43.731942: Current learning rate: 0.0003
2025-08-04 18:03:06.112038: train_loss 3.6624
2025-08-04 18:03:06.110227: train_loss 3.6624
2025-08-04 18:03:06.113896: Epoch time: 442.38 s
2025-08-04 18:03:06.113427: Epoch time: 446.13 s
2025-08-04 18:03:06.118987: train_loss 3.6624
2025-08-04 18:03:06.122002: Epoch time: 442.53 s
2025-08-04 18:03:10.963525: 
2025-08-04 18:03:10.963916: Epoch 52
2025-08-04 18:03:10.964489: Current learning rate: 0.0003
2025-08-04 18:03:11.777961: 
2025-08-04 18:03:11.778409: Epoch 52
2025-08-04 18:03:11.779054: Current learning rate: 0.0003
2025-08-04 18:03:12.177369: 
2025-08-04 18:03:12.180859: Epoch 52
2025-08-04 18:03:12.181496: Current learning rate: 0.0003
2025-08-04 18:10:32.062588: train_loss 3.6611
2025-08-04 18:10:32.067407: Epoch time: 439.89 s
2025-08-04 18:10:32.068078: train_loss 3.6611
2025-08-04 18:10:32.069544: Epoch time: 441.11 s
2025-08-04 18:10:32.066709: train_loss 3.6611
2025-08-04 18:10:32.076430: Epoch time: 440.29 s
2025-08-04 18:10:37.133258: 
2025-08-04 18:10:37.136246: Epoch 53
2025-08-04 18:10:37.144268: Current learning rate: 0.0003
2025-08-04 18:10:38.349592: 
2025-08-04 18:10:38.359186: Epoch 53
2025-08-04 18:10:38.374128: Current learning rate: 0.0003
2025-08-04 18:10:38.753731: 
2025-08-04 18:10:38.754159: Epoch 53
2025-08-04 18:10:38.755022: Current learning rate: 0.0003
2025-08-04 18:17:52.099662: train_loss 3.6617
2025-08-04 18:17:52.104903: Epoch time: 433.35 s
2025-08-04 18:17:52.098566: train_loss 3.6617
2025-08-04 18:17:52.105527: Epoch time: 433.75 s
2025-08-04 18:17:52.106571: train_loss 3.6617
2025-08-04 18:17:52.115116: Epoch time: 434.97 s
2025-08-04 18:17:57.800922: 
2025-08-04 18:17:57.801319: Epoch 54
2025-08-04 18:17:57.801938: Current learning rate: 0.0003
2025-08-04 18:17:58.294095: 
2025-08-04 18:17:58.296860: Epoch 54
2025-08-04 18:17:58.297676: Current learning rate: 0.0003
2025-08-04 18:17:59.430689: 
2025-08-04 18:17:59.433312: Epoch 54
2025-08-04 18:17:59.434362: Current learning rate: 0.0003
2025-08-04 18:25:24.466993: train_loss 3.6619
2025-08-04 18:25:24.465910: train_loss 3.6619
2025-08-04 18:25:24.469993: Epoch time: 446.67 s
2025-08-04 18:25:24.467763: train_loss 3.6619
2025-08-04 18:25:24.476016: Epoch time: 445.04 s
2025-08-04 18:25:24.468415: Epoch time: 446.17 s
2025-08-04 18:25:29.079057: 
2025-08-04 18:25:29.079448: Epoch 55
2025-08-04 18:25:29.080080: Current learning rate: 0.0003
2025-08-04 18:25:29.910021: 
2025-08-04 18:25:29.912851: Epoch 55
2025-08-04 18:25:29.916472: Current learning rate: 0.0003
2025-08-04 18:25:30.938289: 
2025-08-04 18:25:30.942039: Epoch 55
2025-08-04 18:25:30.942750: Current learning rate: 0.0003
2025-08-04 18:32:47.259707: train_loss 3.6593
2025-08-04 18:32:47.260910: Epoch time: 436.32 s
2025-08-04 18:32:47.268285: train_loss 3.6593
2025-08-04 18:32:47.270971: Epoch time: 437.36 s
2025-08-04 18:32:47.267959: train_loss 3.6593
2025-08-04 18:32:47.272288: Epoch time: 438.19 s
2025-08-04 18:32:51.582483: 
2025-08-04 18:32:51.586010: Epoch 56
2025-08-04 18:32:51.586693: Current learning rate: 0.0003
2025-08-04 18:32:55.668931: 
2025-08-04 18:32:55.671571: Epoch 56
2025-08-04 18:32:55.674174: Current learning rate: 0.0003
2025-08-04 18:32:55.818351: 
2025-08-04 18:32:55.818732: Epoch 56
2025-08-04 18:32:55.819298: Current learning rate: 0.0003
2025-08-04 18:40:04.933884: train_loss 3.6598
2025-08-04 18:40:04.936584: Epoch time: 429.12 s
2025-08-04 18:40:04.937390: train_loss 3.6598
2025-08-04 18:40:04.938236: train_loss 3.6598
2025-08-04 18:40:04.943868: Epoch time: 429.27 s
2025-08-04 18:40:04.942124: Epoch time: 433.35 s
2025-08-04 18:40:10.491807: 
2025-08-04 18:40:10.492206: Epoch 57
2025-08-04 18:40:10.497902: Current learning rate: 0.0003
2025-08-04 18:40:12.045341: 
2025-08-04 18:40:12.045695: Epoch 57
2025-08-04 18:40:12.046247: Current learning rate: 0.0003
2025-08-04 18:40:12.192224: 
2025-08-04 18:40:12.195054: Epoch 57
2025-08-04 18:40:12.195812: Current learning rate: 0.0003
2025-08-04 18:47:29.290907: train_loss 3.6592
2025-08-04 18:47:29.296621: Epoch time: 437.25 s
2025-08-04 18:47:29.290424: train_loss 3.6592
2025-08-04 18:47:29.291849: train_loss 3.6592
2025-08-04 18:47:29.300368: Epoch time: 437.1 s
2025-08-04 18:47:29.305661: Epoch time: 438.8 s
2025-08-04 18:47:33.380456: 
2025-08-04 18:47:33.381064: Epoch 58
2025-08-04 18:47:33.382120: Current learning rate: 0.0003
2025-08-04 18:47:35.068576: 
2025-08-04 18:47:35.068976: Epoch 58
2025-08-04 18:47:35.069505: Current learning rate: 0.0003
2025-08-04 18:47:35.217405: 
2025-08-04 18:47:35.220043: Epoch 58
2025-08-04 18:47:35.222775: Current learning rate: 0.0003
2025-08-04 18:54:58.586200: train_loss 3.6575
2025-08-04 18:54:58.587433: Epoch time: 443.52 s
2025-08-04 18:54:58.592969: train_loss 3.6575
2025-08-04 18:54:58.589242: train_loss 3.6575
2025-08-04 18:54:58.594429: Epoch time: 443.37 s
2025-08-04 18:54:58.601406: Epoch time: 445.21 s
2025-08-04 18:55:04.284784: 
2025-08-04 18:55:04.285141: Epoch 59
2025-08-04 18:55:04.285738: Current learning rate: 0.0003
2025-08-04 18:55:04.902439: 
2025-08-04 18:55:04.904918: Epoch 59
2025-08-04 18:55:04.905635: Current learning rate: 0.0003
2025-08-04 18:55:07.519347: 
2025-08-04 18:55:07.521980: Epoch 59
2025-08-04 18:55:07.523680: Current learning rate: 0.0003
2025-08-04 19:02:26.099901: train_loss 3.6537
2025-08-04 19:02:26.101193: Epoch time: 441.82 s
2025-08-04 19:02:26.102598: train_loss 3.6537
2025-08-04 19:02:26.105065: Epoch time: 441.2 s
2025-08-04 19:02:26.099370: train_loss 3.6537
2025-08-04 19:02:26.113071: Epoch time: 438.58 s
2025-08-04 19:02:30.805113: 
2025-08-04 19:02:30.805475: Epoch 60
2025-08-04 19:02:30.806079: Current learning rate: 0.0003
2025-08-04 19:02:31.270299: 
2025-08-04 19:02:31.270764: Epoch 60
2025-08-04 19:02:31.271365: Current learning rate: 0.0003
2025-08-04 19:02:32.724035: 
2025-08-04 19:02:32.726646: Epoch 60
2025-08-04 19:02:32.728813: Current learning rate: 0.0003
2025-08-04 19:09:54.239735: train_loss 3.6541
2025-08-04 19:09:54.238271: train_loss 3.6541
2025-08-04 19:09:54.243937: Epoch time: 441.52 s
2025-08-04 19:09:54.239392: train_loss 3.6541
2025-08-04 19:09:54.246725: Epoch time: 443.43 s
2025-08-04 19:09:54.241737: Epoch time: 442.97 s
2025-08-04 19:09:58.360551: 
2025-08-04 19:09:58.360945: Epoch 61
2025-08-04 19:09:58.361600: Current learning rate: 0.0003
2025-08-04 19:10:00.106377: 
2025-08-04 19:10:00.108709: Epoch 61
2025-08-04 19:10:00.109372: Current learning rate: 0.0003
2025-08-04 19:10:00.235892: 
2025-08-04 19:10:00.238167: Epoch 61
2025-08-04 19:10:00.238983: Current learning rate: 0.0003
2025-08-04 19:17:17.534018: train_loss 3.6538
2025-08-04 19:17:17.535802: Epoch time: 437.43 s
2025-08-04 19:17:17.535867: train_loss 3.6538
2025-08-04 19:17:17.540329: Epoch time: 439.18 s
2025-08-04 19:17:17.534853: train_loss 3.6538
2025-08-04 19:17:17.542176: Epoch time: 437.3 s
2025-08-04 19:17:22.603889: 
2025-08-04 19:17:22.605304: Epoch 62
2025-08-04 19:17:22.605948: Current learning rate: 0.0003
2025-08-04 19:17:23.124768: 
2025-08-04 19:17:23.127554: Epoch 62
2025-08-04 19:17:23.128244: Current learning rate: 0.0003
2025-08-04 19:17:24.471178: 
2025-08-04 19:17:24.471641: Epoch 62
2025-08-04 19:17:24.472262: Current learning rate: 0.0003
2025-08-04 19:24:41.331789: train_loss 3.6541
2025-08-04 19:24:41.334910: Epoch time: 438.73 s
2025-08-04 19:24:41.331363: train_loss 3.6541
2025-08-04 19:24:41.340093: Epoch time: 436.86 s
2025-08-04 19:24:41.337965: train_loss 3.6541
2025-08-04 19:24:41.341511: Epoch time: 438.21 s
2025-08-04 19:24:45.207434: 
2025-08-04 19:24:45.207784: Epoch 63
2025-08-04 19:24:45.208765: Current learning rate: 0.0003
2025-08-04 19:24:46.838805: 
2025-08-04 19:24:46.841607: Epoch 63
2025-08-04 19:24:46.842215: Current learning rate: 0.0003
2025-08-04 19:24:47.004755: 
2025-08-04 19:24:47.007155: Epoch 63
2025-08-04 19:24:47.007969: Current learning rate: 0.0003
2025-08-04 19:32:09.130536: train_loss 3.6521
2025-08-04 19:32:09.133351: Epoch time: 442.13 s
2025-08-04 19:32:09.130271: train_loss 3.6521
2025-08-04 19:32:09.136672: Epoch time: 443.92 s
2025-08-04 19:32:09.136661: train_loss 3.6521
2025-08-04 19:32:09.143310: Epoch time: 442.3 s
2025-08-04 19:32:14.386805: 
2025-08-04 19:32:14.389272: Epoch 64
2025-08-04 19:32:14.396896: Current learning rate: 0.0003
2025-08-04 19:32:14.769084: 
2025-08-04 19:32:14.771869: Epoch 64
2025-08-04 19:32:14.772524: Current learning rate: 0.0003
2025-08-04 19:32:15.257642: 
2025-08-04 19:32:15.257999: Epoch 64
2025-08-04 19:32:15.258608: Current learning rate: 0.0003
2025-08-04 19:39:28.905982: train_loss 3.6514
2025-08-04 19:39:28.907593: Epoch time: 433.65 s
2025-08-04 19:39:28.907169: train_loss 3.6514
2025-08-04 19:39:28.908603: Epoch time: 434.52 s
2025-08-04 19:39:28.907577: train_loss 3.6514
2025-08-04 19:39:28.916614: Epoch time: 434.14 s
2025-08-04 19:39:35.054912: 
2025-08-04 19:39:35.055734: Epoch 65
2025-08-04 19:39:35.056344: Current learning rate: 0.0003
2025-08-04 19:39:37.641387: 
2025-08-04 19:39:37.643767: Epoch 65
2025-08-04 19:39:37.644644: Current learning rate: 0.0003
2025-08-04 19:39:37.908842: 
2025-08-04 19:39:37.909268: Epoch 65
2025-08-04 19:39:37.909875: Current learning rate: 0.0003
2025-08-04 19:46:54.562168: train_loss 3.6501
2025-08-04 19:46:54.568488: Epoch time: 436.92 s
2025-08-04 19:46:54.566576: train_loss 3.6501
2025-08-04 19:46:54.571696: Epoch time: 436.66 s
2025-08-04 19:46:54.574588: train_loss 3.6501
2025-08-04 19:46:54.585986: Epoch time: 439.52 s
2025-08-04 19:46:59.658693: 
2025-08-04 19:46:59.660115: Epoch 66
2025-08-04 19:46:59.660703: Current learning rate: 0.0003
2025-08-04 19:47:00.016209: 
2025-08-04 19:47:00.016570: Epoch 66
2025-08-04 19:47:00.017167: Current learning rate: 0.0003
2025-08-04 19:47:03.082935: 
2025-08-04 19:47:03.085294: Epoch 66
2025-08-04 19:47:03.095841: Current learning rate: 0.0003
2025-08-04 19:54:31.673253: train_loss 3.6486
2025-08-04 19:54:31.674595: Epoch time: 448.59 s
2025-08-04 19:54:31.673398: train_loss 3.6486
2025-08-04 19:54:31.678103: Epoch time: 451.66 s
2025-08-04 19:54:31.682831: train_loss 3.6486
2025-08-04 19:54:31.685243: Epoch time: 452.02 s
2025-08-04 19:54:36.525905: 
2025-08-04 19:54:36.526307: Epoch 67
2025-08-04 19:54:36.526879: Current learning rate: 0.0003
2025-08-04 19:54:36.862453: 
2025-08-04 19:54:36.864797: Epoch 67
2025-08-04 19:54:36.865646: Current learning rate: 0.0003
2025-08-04 19:54:38.644601: 
2025-08-04 19:54:38.646874: Epoch 67
2025-08-04 19:54:38.647479: Current learning rate: 0.0003
2025-08-04 20:01:54.728055: train_loss 3.6489
2025-08-04 20:01:54.730304: train_loss 3.6489
2025-08-04 20:01:54.735205: Epoch time: 438.21 s
2025-08-04 20:01:54.729988: train_loss 3.6489
2025-08-04 20:01:54.732272: Epoch time: 437.87 s
2025-08-04 20:01:54.737345: Epoch time: 436.09 s
2025-08-04 20:02:00.416392: 
2025-08-04 20:02:00.418671: Epoch 68
2025-08-04 20:02:00.430530: Current learning rate: 0.0003
2025-08-04 20:02:02.935963: 
2025-08-04 20:02:02.938305: Epoch 68
2025-08-04 20:02:02.941734: Current learning rate: 0.0003
2025-08-04 20:02:03.044095: 
2025-08-04 20:02:03.044483: Epoch 68
2025-08-04 20:02:03.045020: Current learning rate: 0.0003
2025-08-04 20:09:27.970733: train_loss 3.6476
2025-08-04 20:09:27.971960: train_loss 3.6476
2025-08-04 20:09:27.972141: train_loss 3.6476
2025-08-04 20:09:27.976772: Epoch time: 444.93 s
2025-08-04 20:09:27.974725: Epoch time: 445.04 s
2025-08-04 20:09:27.977684: Epoch time: 447.56 s
2025-08-04 20:09:32.370229: 
2025-08-04 20:09:32.371039: Epoch 69
2025-08-04 20:09:32.371738: Current learning rate: 0.0003
2025-08-04 20:09:34.146695: 
2025-08-04 20:09:34.147359: Epoch 69
2025-08-04 20:09:34.147943: Current learning rate: 0.0003
2025-08-04 20:09:34.270828: 
2025-08-04 20:09:34.273654: Epoch 69
2025-08-04 20:09:34.275348: Current learning rate: 0.0003
2025-08-04 20:16:46.303611: train_loss 3.6483
2025-08-04 20:16:46.310244: Epoch time: 432.16 s
2025-08-04 20:16:46.310993: train_loss 3.6483
2025-08-04 20:16:46.312185: Epoch time: 433.94 s
2025-08-04 20:16:46.317886: train_loss 3.6483
2025-08-04 20:16:46.326680: Epoch time: 432.05 s
2025-08-04 20:16:51.910043: 
2025-08-04 20:16:51.912592: Epoch 70
2025-08-04 20:16:51.913184: Current learning rate: 0.0003
2025-08-04 20:16:55.564475: 
2025-08-04 20:16:55.567580: Epoch 70
2025-08-04 20:16:55.569271: Current learning rate: 0.0003
2025-08-04 20:16:55.633305: 
2025-08-04 20:16:55.633669: Epoch 70
2025-08-04 20:16:55.634232: Current learning rate: 0.0003
2025-08-04 20:24:29.294812: train_loss 3.6478
2025-08-04 20:24:29.296467: Epoch time: 457.38 s
2025-08-04 20:24:29.292342: train_loss 3.6478
2025-08-04 20:24:29.302277: Epoch time: 453.73 s
2025-08-04 20:24:29.303304: train_loss 3.6478
2025-08-04 20:24:29.305605: Epoch time: 453.66 s
2025-08-04 20:24:33.824736: 
2025-08-04 20:24:33.825120: Epoch 71
2025-08-04 20:24:33.825950: Current learning rate: 0.0003
2025-08-04 20:24:36.637730: 
2025-08-04 20:24:36.638200: Epoch 71
2025-08-04 20:24:36.639223: Current learning rate: 0.0003
2025-08-04 20:24:36.870147: 
2025-08-04 20:24:36.873316: Epoch 71
2025-08-04 20:24:36.874109: Current learning rate: 0.0003
2025-08-04 20:31:53.262456: train_loss 3.6475
2025-08-04 20:31:53.268130: Epoch time: 436.39 s
2025-08-04 20:31:53.262415: train_loss 3.6475
2025-08-04 20:31:53.271618: Epoch time: 439.44 s
2025-08-04 20:31:53.268154: train_loss 3.6475
2025-08-04 20:31:53.279048: Epoch time: 436.63 s
2025-08-04 20:31:57.833871: 
2025-08-04 20:31:57.837555: Epoch 72
2025-08-04 20:31:57.848531: Current learning rate: 0.0003
2025-08-04 20:32:01.686974: 
2025-08-04 20:32:01.689646: Epoch 72
2025-08-04 20:32:01.691739: Current learning rate: 0.0003
2025-08-04 20:32:01.898847: 
2025-08-04 20:32:01.899235: Epoch 72
2025-08-04 20:32:01.899784: Current learning rate: 0.0003
2025-08-04 20:39:15.653136: train_loss 3.6479
2025-08-04 20:39:15.653398: train_loss 3.6479
2025-08-04 20:39:15.659699: Epoch time: 433.75 s
2025-08-04 20:39:15.657558: Epoch time: 437.82 s
2025-08-04 20:39:15.654355: train_loss 3.6479
2025-08-04 20:39:15.668484: Epoch time: 433.97 s
2025-08-04 20:39:20.386158: 
2025-08-04 20:39:20.386532: Epoch 73
2025-08-04 20:39:20.387172: Current learning rate: 0.0003
2025-08-04 20:39:24.283120: 
2025-08-04 20:39:24.285624: Epoch 73
2025-08-04 20:39:24.286241: Current learning rate: 0.0003
2025-08-04 20:39:24.292076: 
2025-08-04 20:39:24.294384: Epoch 73
2025-08-04 20:39:24.295007: Current learning rate: 0.0003
2025-08-04 20:46:45.705969: train_loss 3.6461
2025-08-04 20:46:45.711192: Epoch time: 441.41 s
2025-08-04 20:46:45.711906: train_loss 3.6461
2025-08-04 20:46:45.712510: train_loss 3.6461
2025-08-04 20:46:45.718484: Epoch time: 445.33 s
2025-08-04 20:46:45.717846: Epoch time: 441.42 s
2025-08-04 20:46:49.733931: 
2025-08-04 20:46:49.734333: Epoch 74
2025-08-04 20:46:49.734941: Current learning rate: 0.0003
2025-08-04 20:46:51.233048: 
2025-08-04 20:46:51.235239: Epoch 74
2025-08-04 20:46:51.236851: Current learning rate: 0.0003
2025-08-04 20:46:51.350787: 
2025-08-04 20:46:51.352847: Epoch 74
2025-08-04 20:46:51.353513: Current learning rate: 0.0003
2025-08-04 20:54:13.365618: train_loss 3.6452
2025-08-04 20:54:13.367015: Epoch time: 442.02 s
2025-08-04 20:54:13.366492: train_loss 3.6452
2025-08-04 20:54:13.372644: Epoch time: 442.13 s
2025-08-04 20:54:13.369236: train_loss 3.6452
2025-08-04 20:54:13.378293: Epoch time: 443.63 s
2025-08-04 20:54:17.535500: Saving checkpoint at epoch 75...
2025-08-04 20:54:19.190301: Saving checkpoint at epoch 75...
2025-08-04 20:54:19.287467: Saving checkpoint at epoch 75...
2025-08-04 20:54:20.989428: 
2025-08-04 20:54:20.989779: Epoch 75
2025-08-04 20:54:20.990373: Current learning rate: 0.0003
2025-08-04 20:54:22.998500: 
2025-08-04 20:54:22.998864: Epoch 75
2025-08-04 20:54:22.999560: Current learning rate: 0.0003
2025-08-04 20:54:23.788250: 
2025-08-04 20:54:23.793956: Epoch 75
2025-08-04 20:54:23.794731: Current learning rate: 0.0003
2025-08-04 21:01:37.002642: train_loss 3.6435
2025-08-04 21:01:37.005969: Epoch time: 433.22 s
2025-08-04 21:01:37.002930: train_loss 3.6435
2025-08-04 21:01:37.011844: Epoch time: 436.01 s
2025-08-04 21:01:37.009646: train_loss 3.6435
2025-08-04 21:01:37.019699: Epoch time: 434.01 s
2025-08-04 21:01:41.732680: 
2025-08-04 21:01:41.733115: Epoch 76
2025-08-04 21:01:41.733807: Current learning rate: 0.0003
2025-08-04 21:01:42.620977: 
2025-08-04 21:01:42.621446: Epoch 76
2025-08-04 21:01:42.622054: Current learning rate: 0.0003
2025-08-04 21:01:42.908678: 
2025-08-04 21:01:42.911075: Epoch 76
2025-08-04 21:01:42.912235: Current learning rate: 0.0003
2025-08-04 21:09:00.855568: train_loss 3.6443
2025-08-04 21:09:00.859071: train_loss 3.6443
2025-08-04 21:09:00.861897: Epoch time: 439.13 s
2025-08-04 21:09:00.856872: train_loss 3.6443
2025-08-04 21:09:00.862436: Epoch time: 437.95 s
2025-08-04 21:09:00.860832: Epoch time: 438.24 s
2025-08-04 21:09:06.713941: 
2025-08-04 21:09:06.716564: Epoch 77
2025-08-04 21:09:06.718462: Current learning rate: 0.0003
2025-08-04 21:09:08.629487: 
2025-08-04 21:09:08.629834: Epoch 77
2025-08-04 21:09:08.630408: Current learning rate: 0.0003
2025-08-04 21:09:14.012466: 
2025-08-04 21:09:14.014973: Epoch 77
2025-08-04 21:09:14.015718: Current learning rate: 0.0003
2025-08-04 21:16:33.831003: train_loss 3.6438
2025-08-04 21:16:33.832738: Epoch time: 447.12 s
2025-08-04 21:16:33.836542: train_loss 3.6438
2025-08-04 21:16:33.843590: Epoch time: 445.21 s
2025-08-04 21:16:33.838204: train_loss 3.6438
2025-08-04 21:16:33.846954: Epoch time: 439.83 s
2025-08-04 21:16:38.620664: 
2025-08-04 21:16:38.621135: Epoch 78
2025-08-04 21:16:38.621770: Current learning rate: 0.0003
2025-08-04 21:16:40.371939: 
2025-08-04 21:16:40.372348: Epoch 78
2025-08-04 21:16:40.372931: Current learning rate: 0.0003
2025-08-04 21:16:40.490146: 
2025-08-04 21:16:40.493016: Epoch 78
2025-08-04 21:16:40.495047: Current learning rate: 0.0003
2025-08-04 21:23:58.494841: train_loss 3.6419
2025-08-04 21:23:58.494928: train_loss 3.6419
2025-08-04 21:23:58.496000: Epoch time: 438.12 s
2025-08-04 21:23:58.496148: Epoch time: 439.88 s
2025-08-04 21:23:58.500942: train_loss 3.6419
2025-08-04 21:23:58.503728: Epoch time: 438.01 s
2025-08-04 21:24:03.194118: 
2025-08-04 21:24:03.196592: Epoch 79
2025-08-04 21:24:03.197223: Current learning rate: 0.0003
2025-08-04 21:24:03.701114: 
2025-08-04 21:24:03.701463: Epoch 79
2025-08-04 21:24:03.702066: Current learning rate: 0.0003
2025-08-04 21:24:04.863788: 
2025-08-04 21:24:04.866330: Epoch 79
2025-08-04 21:24:04.867054: Current learning rate: 0.0003
2025-08-04 21:31:18.101279: train_loss 3.6436
2025-08-04 21:31:18.096408: train_loss 3.6436
2025-08-04 21:31:18.104162: Epoch time: 434.9 s
2025-08-04 21:31:18.102569: Epoch time: 433.24 s
2025-08-04 21:31:18.095716: train_loss 3.6436
2025-08-04 21:31:18.109470: Epoch time: 434.4 s
2025-08-04 21:31:23.249424: 
2025-08-04 21:31:23.249789: Epoch 80
2025-08-04 21:31:23.250364: Current learning rate: 0.0003
2025-08-04 21:31:25.630494: 
2025-08-04 21:31:25.633030: Epoch 80
2025-08-04 21:31:25.633662: Current learning rate: 0.0003
2025-08-04 21:31:25.768249: 
2025-08-04 21:31:25.770478: Epoch 80
2025-08-04 21:31:25.771301: Current learning rate: 0.0003
2025-08-04 21:38:42.917038: train_loss 3.6424
2025-08-04 21:38:42.918644: Epoch time: 439.67 s
2025-08-04 21:38:42.920557: train_loss 3.6424
2025-08-04 21:38:42.919397: train_loss 3.6424
2025-08-04 21:38:42.924330: Epoch time: 437.29 s
2025-08-04 21:38:42.929714: Epoch time: 437.15 s
2025-08-04 21:38:50.621368: 
2025-08-04 21:38:50.623938: Epoch 81
2025-08-04 21:38:50.629097: Current learning rate: 0.0003
2025-08-04 21:38:51.494777: 
2025-08-04 21:38:51.495179: Epoch 81
2025-08-04 21:38:51.495758: Current learning rate: 0.0003
2025-08-04 21:38:51.594937: 
2025-08-04 21:38:51.597420: Epoch 81
2025-08-04 21:38:51.598423: Current learning rate: 0.0003
2025-08-04 21:46:08.184888: train_loss 3.64
2025-08-04 21:46:08.187075: train_loss 3.64
2025-08-04 21:46:08.191558: Epoch time: 436.69 s
2025-08-04 21:46:08.188357: train_loss 3.64
2025-08-04 21:46:08.186906: Epoch time: 437.56 s
2025-08-04 21:46:08.194114: Epoch time: 436.59 s
2025-08-04 21:46:13.390501: 
2025-08-04 21:46:13.390888: Epoch 82
2025-08-04 21:46:13.391454: Current learning rate: 0.0003
2025-08-04 21:46:14.509297: 
2025-08-04 21:46:14.511801: Epoch 82
2025-08-04 21:46:14.512963: Current learning rate: 0.0003
2025-08-04 21:46:14.792969: 
2025-08-04 21:46:14.795382: Epoch 82
2025-08-04 21:46:14.796448: Current learning rate: 0.0003
2025-08-04 21:53:34.936899: train_loss 3.6418
2025-08-04 21:53:34.942077: train_loss 3.6418
2025-08-04 21:53:34.938766: Epoch time: 440.43 s
2025-08-04 21:53:34.946289: Epoch time: 440.15 s
2025-08-04 21:53:34.944031: train_loss 3.6418
2025-08-04 21:53:34.949090: Epoch time: 441.55 s
2025-08-04 21:53:40.264403: 
2025-08-04 21:53:40.267465: Epoch 83
2025-08-04 21:53:40.269245: Current learning rate: 0.00029
2025-08-04 21:53:41.743947: 
2025-08-04 21:53:41.744297: Epoch 83
2025-08-04 21:53:41.745002: Current learning rate: 0.00029
2025-08-04 21:53:41.841981: 
2025-08-04 21:53:41.844580: Epoch 83
2025-08-04 21:53:41.845279: Current learning rate: 0.00029
2025-08-04 22:01:06.556824: train_loss 3.6396
2025-08-04 22:01:06.558058: Epoch time: 444.72 s
2025-08-04 22:01:06.558283: train_loss 3.6396
2025-08-04 22:01:06.562019: Epoch time: 444.81 s
2025-08-04 22:01:06.559648: train_loss 3.6396
2025-08-04 22:01:06.564932: Epoch time: 446.3 s
2025-08-04 22:01:11.431254: 
2025-08-04 22:01:11.434280: Epoch 84
2025-08-04 22:01:11.440739: Current learning rate: 0.00029
2025-08-04 22:01:12.819515: 
2025-08-04 22:01:12.822329: Epoch 84
2025-08-04 22:01:12.823163: Current learning rate: 0.00029
2025-08-04 22:01:12.972936: 
2025-08-04 22:01:12.973357: Epoch 84
2025-08-04 22:01:12.974195: Current learning rate: 0.00029
2025-08-04 22:08:30.000452: train_loss 3.64
2025-08-04 22:08:30.003070: Epoch time: 437.18 s
2025-08-04 22:08:30.000262: train_loss 3.64
2025-08-04 22:08:30.008688: Epoch time: 437.03 s
2025-08-04 22:08:30.010823: train_loss 3.64
2025-08-04 22:08:30.013738: Epoch time: 438.58 s
2025-08-04 22:08:35.510438: 
2025-08-04 22:08:35.510868: Epoch 85
2025-08-04 22:08:35.511500: Current learning rate: 0.00029
2025-08-04 22:08:36.963185: 
2025-08-04 22:08:36.965411: Epoch 85
2025-08-04 22:08:36.965986: Current learning rate: 0.00029
2025-08-04 22:08:37.082726: 
2025-08-04 22:08:37.083112: Epoch 85
2025-08-04 22:08:37.083692: Current learning rate: 0.00029
2025-08-04 22:16:05.812150: train_loss 3.6406
2025-08-04 22:16:05.816524: Epoch time: 448.73 s
2025-08-04 22:16:05.814858: train_loss 3.6406
2025-08-04 22:16:05.814224: train_loss 3.6406
2025-08-04 22:16:05.821769: Epoch time: 450.31 s
2025-08-04 22:16:05.822846: Epoch time: 448.85 s
2025-08-04 22:16:09.825675: 
2025-08-04 22:16:09.827196: Epoch 86
2025-08-04 22:16:09.832448: Current learning rate: 0.00029
2025-08-04 22:16:10.817890: 
2025-08-04 22:16:10.818239: Epoch 86
2025-08-04 22:16:10.818798: Current learning rate: 0.00029
2025-08-04 22:16:13.755767: 
2025-08-04 22:16:13.757958: Epoch 86
2025-08-04 22:16:13.764235: Current learning rate: 0.00029
2025-08-04 22:23:33.427952: train_loss 3.6405
2025-08-04 22:23:33.430959: Epoch time: 443.6 s
2025-08-04 22:23:33.434005: train_loss 3.6405
2025-08-04 22:23:33.435147: Epoch time: 442.61 s
2025-08-04 22:23:33.430349: train_loss 3.6405
2025-08-04 22:23:33.445504: Epoch time: 439.68 s
2025-08-04 22:23:37.807848: 
2025-08-04 22:23:37.811352: Epoch 87
2025-08-04 22:23:37.901134: Current learning rate: 0.00029
2025-08-04 22:23:41.112569: 
2025-08-04 22:23:41.112892: Epoch 87
2025-08-04 22:23:41.113426: Current learning rate: 0.00029
2025-08-04 22:23:41.351349: 
2025-08-04 22:23:41.364649: Epoch 87
2025-08-04 22:23:41.368298: Current learning rate: 0.00029
2025-08-04 22:31:03.232724: train_loss 3.6379
2025-08-04 22:31:03.234775: train_loss 3.6379
2025-08-04 22:31:03.236121: Epoch time: 441.88 s
2025-08-04 22:31:03.236968: Epoch time: 442.12 s
2025-08-04 22:31:03.235703: train_loss 3.6379
2025-08-04 22:31:03.241123: Epoch time: 445.42 s
2025-08-04 22:31:11.346906: 
2025-08-04 22:31:11.347321: Epoch 88
2025-08-04 22:31:11.347970: Current learning rate: 0.00029
2025-08-04 22:31:13.428321: 
2025-08-04 22:31:13.428899: Epoch 88
2025-08-04 22:31:13.429474: Current learning rate: 0.00029
2025-08-04 22:31:13.525696: 
2025-08-04 22:31:13.527843: Epoch 88
2025-08-04 22:31:13.528910: Current learning rate: 0.00029
2025-08-04 22:38:46.154574: train_loss 3.6357
2025-08-04 22:38:46.154849: train_loss 3.6357
2025-08-04 22:38:46.159904: Epoch time: 452.73 s
2025-08-04 22:38:46.161115: Epoch time: 452.63 s
2025-08-04 22:38:46.154939: train_loss 3.6357
2025-08-04 22:38:46.166276: Epoch time: 454.81 s
2025-08-04 22:38:50.978605: 
2025-08-04 22:38:50.981466: Epoch 89
2025-08-04 22:38:50.982126: Current learning rate: 0.00029
2025-08-04 22:38:51.633137: 
2025-08-04 22:38:51.635305: Epoch 89
2025-08-04 22:38:51.635969: Current learning rate: 0.00029
2025-08-04 22:38:51.773047: 
2025-08-04 22:38:51.773391: Epoch 89
2025-08-04 22:38:51.773923: Current learning rate: 0.00029
2025-08-04 22:46:01.476462: train_loss 3.6399
2025-08-04 22:46:01.479832: Epoch time: 429.84 s
2025-08-04 22:46:01.474378: train_loss 3.6399
2025-08-04 22:46:01.483139: Epoch time: 429.7 s
2025-08-04 22:46:01.479573: train_loss 3.6399
2025-08-04 22:46:01.490290: Epoch time: 430.5 s
2025-08-04 22:46:06.685070: 
2025-08-04 22:46:06.685449: Epoch 90
2025-08-04 22:46:06.686019: Current learning rate: 0.00029
2025-08-04 22:46:07.234358: 
2025-08-04 22:46:07.235260: Epoch 90
2025-08-04 22:46:07.235836: Current learning rate: 0.00029
2025-08-04 22:46:07.717466: 
2025-08-04 22:46:07.719441: Epoch 90
2025-08-04 22:46:07.720170: Current learning rate: 0.00029
2025-08-04 22:53:28.638721: train_loss 3.6358
2025-08-04 22:53:28.639782: train_loss 3.6358
2025-08-04 22:53:28.641405: Epoch time: 441.96 s
2025-08-04 22:53:28.640233: train_loss 3.6358
2025-08-04 22:53:28.643186: Epoch time: 441.41 s
2025-08-04 22:53:28.641243: Epoch time: 440.92 s
2025-08-04 22:53:33.939483: 
2025-08-04 22:53:33.939822: Epoch 91
2025-08-04 22:53:33.940434: Current learning rate: 0.00029
2025-08-04 22:53:38.223103: 
2025-08-04 22:53:38.240076: Epoch 91
2025-08-04 22:53:38.242808: Current learning rate: 0.00029
2025-08-04 22:53:43.579463: 
2025-08-04 22:53:43.581990: Epoch 91
2025-08-04 22:53:43.583025: Current learning rate: 0.00029
2025-08-04 23:01:08.190860: train_loss 3.634
2025-08-04 23:01:08.192254: Epoch time: 449.97 s
2025-08-04 23:01:08.189815: train_loss 3.634
2025-08-04 23:01:08.194300: train_loss 3.634
2025-08-04 23:01:08.196984: Epoch time: 454.25 s
2025-08-04 23:01:08.198203: Epoch time: 444.62 s
2025-08-04 23:01:12.105261: 
2025-08-04 23:01:12.105647: Epoch 92
2025-08-04 23:01:12.106258: Current learning rate: 0.00029
2025-08-04 23:01:12.746092: 
2025-08-04 23:01:12.748346: Epoch 92
2025-08-04 23:01:12.750581: Current learning rate: 0.00029
2025-08-04 23:01:13.454836: 
2025-08-04 23:01:13.457089: Epoch 92
2025-08-04 23:01:13.457865: Current learning rate: 0.00029
2025-08-04 23:08:29.372346: train_loss 3.6377
2025-08-04 23:08:29.374777: Epoch time: 436.63 s
2025-08-04 23:08:29.374936: train_loss 3.6377
2025-08-04 23:08:29.379418: Epoch time: 437.27 s
2025-08-04 23:08:29.374703: train_loss 3.6377
2025-08-04 23:08:29.384402: Epoch time: 435.92 s
2025-08-04 23:08:34.693932: 
2025-08-04 23:08:34.696190: Epoch 93
2025-08-04 23:08:34.697521: Current learning rate: 0.00029
2025-08-04 23:08:35.515577: 
2025-08-04 23:08:35.517713: Epoch 93
2025-08-04 23:08:35.518475: Current learning rate: 0.00029
2025-08-04 23:08:41.945465: 
2025-08-04 23:08:41.946332: Epoch 93
2025-08-04 23:08:41.946882: Current learning rate: 0.00029
2025-08-04 23:15:58.934840: train_loss 3.6336
2025-08-04 23:15:58.934999: train_loss 3.6336
2025-08-04 23:15:58.937440: Epoch time: 436.99 s
2025-08-04 23:15:58.932846: train_loss 3.6336
2025-08-04 23:15:58.939049: Epoch time: 443.42 s
2025-08-04 23:15:58.936203: Epoch time: 444.24 s
2025-08-04 23:16:03.686364: 
2025-08-04 23:16:03.686786: Epoch 94
2025-08-04 23:16:03.687367: Current learning rate: 0.00029
2025-08-04 23:16:05.212792: 
2025-08-04 23:16:05.213239: Epoch 94
2025-08-04 23:16:05.213931: Current learning rate: 0.00029
2025-08-04 23:16:06.661303: 
2025-08-04 23:16:06.664062: Epoch 94
2025-08-04 23:16:06.664846: Current learning rate: 0.00029
2025-08-04 23:23:30.783068: train_loss 3.6334
2025-08-04 23:23:30.790808: Epoch time: 445.57 s
2025-08-04 23:23:30.786238: train_loss 3.6334
2025-08-04 23:23:30.795182: Epoch time: 447.1 s
2025-08-04 23:23:30.785130: train_loss 3.6334
2025-08-04 23:23:30.808568: Epoch time: 444.12 s
2025-08-04 23:23:35.685521: 
2025-08-04 23:23:35.688377: Epoch 95
2025-08-04 23:23:35.695412: Current learning rate: 0.00029
2025-08-04 23:23:38.276076: 
2025-08-04 23:23:38.278710: Epoch 95
2025-08-04 23:23:38.280600: Current learning rate: 0.00029
2025-08-04 23:23:38.301510: 
2025-08-04 23:23:38.301868: Epoch 95
2025-08-04 23:23:38.302440: Current learning rate: 0.00029
2025-08-04 23:30:56.310932: train_loss 3.6341
2025-08-04 23:30:56.312893: Epoch time: 440.63 s
2025-08-04 23:30:56.311994: train_loss 3.6341
2025-08-04 23:30:56.318092: Epoch time: 438.04 s
2025-08-04 23:30:56.317245: train_loss 3.6341
2025-08-04 23:30:56.320707: Epoch time: 438.01 s
2025-08-04 23:31:01.609496: 
2025-08-04 23:31:01.609892: Epoch 96
2025-08-04 23:31:01.610462: Current learning rate: 0.00029
2025-08-04 23:31:03.264914: 
2025-08-04 23:31:03.267756: Epoch 96
2025-08-04 23:31:03.268535: Current learning rate: 0.00029
2025-08-04 23:31:03.480338: 
2025-08-04 23:31:03.482605: Epoch 96
2025-08-04 23:31:03.483500: Current learning rate: 0.00029
2025-08-04 23:38:33.538993: train_loss 3.6335
2025-08-04 23:38:33.540761: Epoch time: 450.06 s
2025-08-04 23:38:33.541152: train_loss 3.6335
2025-08-04 23:38:33.539999: train_loss 3.6335
2025-08-04 23:38:33.548728: Epoch time: 451.93 s
2025-08-04 23:38:33.546935: Epoch time: 450.28 s
2025-08-04 23:38:38.729489: 
2025-08-04 23:38:38.729891: Epoch 97
2025-08-04 23:38:38.730493: Current learning rate: 0.00029
2025-08-04 23:38:39.085591: 
2025-08-04 23:38:39.086347: Epoch 97
2025-08-04 23:38:39.086906: Current learning rate: 0.00029
2025-08-04 23:38:39.747482: 
2025-08-04 23:38:39.750083: Epoch 97
2025-08-04 23:38:39.750759: Current learning rate: 0.00029
2025-08-04 23:46:09.512043: train_loss 3.6331
2025-08-04 23:46:09.512576: train_loss 3.6331
2025-08-04 23:46:09.513484: Epoch time: 450.43 s
2025-08-04 23:46:09.515502: Epoch time: 449.77 s
2025-08-04 23:46:09.515003: train_loss 3.6331
2025-08-04 23:46:09.517884: Epoch time: 450.79 s
2025-08-04 23:46:13.332046: 
2025-08-04 23:46:13.334096: Epoch 98
2025-08-04 23:46:13.334752: Current learning rate: 0.00029
2025-08-04 23:46:14.209984: 
2025-08-04 23:46:14.212044: Epoch 98
2025-08-04 23:46:14.212757: Current learning rate: 0.00029
2025-08-04 23:46:15.646256: 
2025-08-04 23:46:15.646647: Epoch 98
2025-08-04 23:46:15.647223: Current learning rate: 0.00029
2025-08-04 23:53:26.400486: train_loss 3.6334
2025-08-04 23:53:26.402618: Epoch time: 430.76 s
2025-08-04 23:53:26.398600: train_loss 3.6334
2025-08-04 23:53:26.404108: train_loss 3.6334
2025-08-04 23:53:26.404563: Epoch time: 433.07 s
2025-08-04 23:53:26.406854: Epoch time: 432.2 s
2025-08-04 23:53:31.415214: 
2025-08-04 23:53:31.415723: Epoch 99
2025-08-04 23:53:31.421703: Current learning rate: 0.00029
2025-08-04 23:53:33.665433: 
2025-08-04 23:53:33.665868: Epoch 99
2025-08-04 23:53:33.666555: Current learning rate: 0.00029
2025-08-04 23:53:33.723122: 
2025-08-04 23:53:33.725701: Epoch 99
2025-08-04 23:53:33.727516: Current learning rate: 0.00029
2025-08-05 00:00:51.842974: train_loss 3.6332
2025-08-05 00:00:51.842075: train_loss 3.6332
2025-08-05 00:00:51.850542: Epoch time: 438.18 s
2025-08-05 00:00:51.850345: Epoch time: 440.43 s
2025-08-05 00:00:51.841957: train_loss 3.6332
2025-08-05 00:00:51.860193: Epoch time: 438.12 s
2025-08-05 00:00:55.188429: Saving checkpoint at epoch 100...
2025-08-05 00:00:56.973696: Saving checkpoint at epoch 100...
2025-08-05 00:00:57.170766: Saving checkpoint at epoch 100...
2025-08-05 00:00:59.382958: 
2025-08-05 00:00:59.383327: Epoch 100
2025-08-05 00:00:59.383926: Current learning rate: 0.00029
2025-08-05 00:01:02.365167: 
2025-08-05 00:01:02.365505: Epoch 100
2025-08-05 00:01:02.366128: Current learning rate: 0.00029
2025-08-05 00:01:02.690116: 
2025-08-05 00:01:02.692609: Epoch 100
2025-08-05 00:01:02.693410: Current learning rate: 0.00029
2025-08-05 00:08:28.931845: train_loss 3.6329
2025-08-05 00:08:28.936618: Epoch time: 446.57 s
2025-08-05 00:08:28.938176: train_loss 3.6329
2025-08-05 00:08:28.940507: Epoch time: 449.56 s
2025-08-05 00:08:28.939882: train_loss 3.6329
2025-08-05 00:08:28.949821: Epoch time: 446.25 s
2025-08-05 00:08:34.777287: 
2025-08-05 00:08:34.777700: Epoch 101
2025-08-05 00:08:34.778358: Current learning rate: 0.00029
2025-08-05 00:08:36.991615: 
2025-08-05 00:08:36.991946: Epoch 101
2025-08-05 00:08:36.992512: Current learning rate: 0.00029
2025-08-05 00:08:37.138059: 
2025-08-05 00:08:37.140377: Epoch 101
2025-08-05 00:08:37.141151: Current learning rate: 0.00029
2025-08-05 00:15:51.012722: train_loss 3.6324
2025-08-05 00:15:51.010431: train_loss 3.6324
2025-08-05 00:15:51.008940: train_loss 3.6324
2025-08-05 00:15:51.015689: Epoch time: 433.87 s
2025-08-05 00:15:51.016675: Epoch time: 434.02 s
2025-08-05 00:15:51.020380: Epoch time: 436.23 s
2025-08-05 00:15:55.053108: 
2025-08-05 00:15:55.055240: Epoch 102
2025-08-05 00:15:55.057696: Current learning rate: 0.00029
2025-08-05 00:15:56.761054: 
2025-08-05 00:15:56.761399: Epoch 102
2025-08-05 00:15:56.762021: Current learning rate: 0.00029
2025-08-05 00:15:59.903995: 
2025-08-05 00:15:59.906339: Epoch 102
2025-08-05 00:15:59.907077: Current learning rate: 0.00029
2025-08-05 00:23:23.242996: train_loss 3.6293
2025-08-05 00:23:23.245785: Epoch time: 443.34 s
2025-08-05 00:23:23.243579: train_loss 3.6293
2025-08-05 00:23:23.248565: Epoch time: 446.48 s
2025-08-05 00:23:23.243088: train_loss 3.6293
2025-08-05 00:23:23.250131: Epoch time: 448.19 s
2025-08-05 00:23:28.077003: 
2025-08-05 00:23:28.077475: Epoch 103
2025-08-05 00:23:28.078053: Current learning rate: 0.00029
2025-08-05 00:23:29.770902: 
2025-08-05 00:23:29.771662: Epoch 103
2025-08-05 00:23:29.772483: Current learning rate: 0.00029
2025-08-05 00:23:29.862697: 
2025-08-05 00:23:29.864805: Epoch 103
2025-08-05 00:23:29.865675: Current learning rate: 0.00029
2025-08-05 00:30:44.376520: train_loss 3.6302
2025-08-05 00:30:44.380892: Epoch time: 434.61 s
2025-08-05 00:30:44.380571: train_loss 3.6302
2025-08-05 00:30:44.384593: Epoch time: 434.52 s
2025-08-05 00:30:44.386800: train_loss 3.6302
2025-08-05 00:30:44.390402: Epoch time: 436.31 s
2025-08-05 00:30:51.927191: 
2025-08-05 00:30:51.929363: Epoch 104
2025-08-05 00:30:51.930023: Current learning rate: 0.00029
2025-08-05 00:30:52.874279: 
2025-08-05 00:30:52.874625: Epoch 104
2025-08-05 00:30:52.875188: Current learning rate: 0.00029
2025-08-05 00:30:52.893030: 
2025-08-05 00:30:52.895065: Epoch 104
2025-08-05 00:30:52.895761: Current learning rate: 0.00029
2025-08-05 00:38:10.414677: train_loss 3.6299
2025-08-05 00:38:10.416294: Epoch time: 438.49 s
2025-08-05 00:38:10.423624: train_loss 3.6299
2025-08-05 00:38:10.424812: Epoch time: 437.54 s
2025-08-05 00:38:10.419405: train_loss 3.6299
2025-08-05 00:38:10.431270: Epoch time: 437.53 s
2025-08-05 00:38:14.645629: 
2025-08-05 00:38:14.648281: Epoch 105
2025-08-05 00:38:14.649003: Current learning rate: 0.00029
2025-08-05 00:38:15.601195: 
2025-08-05 00:38:15.601555: Epoch 105
2025-08-05 00:38:15.602136: Current learning rate: 0.00029
2025-08-05 00:38:16.916154: 
2025-08-05 00:38:16.918474: Epoch 105
2025-08-05 00:38:16.922886: Current learning rate: 0.00029
2025-08-05 00:45:41.369100: train_loss 3.6302
2025-08-05 00:45:41.370584: Epoch time: 444.45 s
2025-08-05 00:45:41.372251: train_loss 3.6302
2025-08-05 00:45:41.375152: Epoch time: 446.73 s
2025-08-05 00:45:41.372826: train_loss 3.6302
2025-08-05 00:45:41.382625: Epoch time: 445.77 s
2025-08-05 00:45:45.902542: 
2025-08-05 00:45:45.902962: Epoch 106
2025-08-05 00:45:45.903599: Current learning rate: 0.00029
2025-08-05 00:45:47.717750: 
2025-08-05 00:45:47.720160: Epoch 106
2025-08-05 00:45:47.721223: Current learning rate: 0.00029
2025-08-05 00:45:47.777256: 
2025-08-05 00:45:47.779037: Epoch 106
2025-08-05 00:45:47.779656: Current learning rate: 0.00029
2025-08-05 00:53:14.742945: train_loss 3.6279
2025-08-05 00:53:14.740653: train_loss 3.6279
2025-08-05 00:53:14.744131: Epoch time: 447.02 s
2025-08-05 00:53:14.744741: Epoch time: 448.84 s
2025-08-05 00:53:14.745116: train_loss 3.6279
2025-08-05 00:53:14.747488: Epoch time: 446.97 s
2025-08-05 00:53:19.757741: 
2025-08-05 00:53:19.758672: Epoch 107
2025-08-05 00:53:19.759756: Current learning rate: 0.00029
2025-08-05 00:53:21.359128: 
2025-08-05 00:53:21.359468: Epoch 107
2025-08-05 00:53:21.359986: Current learning rate: 0.00029
2025-08-05 00:53:21.533925: 
2025-08-05 00:53:21.536032: Epoch 107
2025-08-05 00:53:21.536789: Current learning rate: 0.00029
2025-08-05 01:00:44.413460: train_loss 3.6301
2025-08-05 01:00:44.423884: Epoch time: 443.05 s
2025-08-05 01:00:44.418017: train_loss 3.6301
2025-08-05 01:00:44.413524: train_loss 3.6301
2025-08-05 01:00:44.426019: Epoch time: 444.66 s
2025-08-05 01:00:44.425465: Epoch time: 442.89 s
2025-08-05 01:00:48.893850: 
2025-08-05 01:00:48.896619: Epoch 108
2025-08-05 01:00:48.898909: Current learning rate: 0.00029
2025-08-05 01:00:52.133264: 
2025-08-05 01:00:52.133744: Epoch 108
2025-08-05 01:00:52.134332: Current learning rate: 0.00029
2025-08-05 01:00:52.327179: 
2025-08-05 01:00:52.329983: Epoch 108
2025-08-05 01:00:52.330753: Current learning rate: 0.00029
2025-08-05 01:08:03.894475: train_loss 3.6278
2025-08-05 01:08:03.894353: train_loss 3.6278
2025-08-05 01:08:03.900149: Epoch time: 435.0 s
2025-08-05 01:08:03.897719: Epoch time: 431.57 s
2025-08-05 01:08:03.902470: train_loss 3.6278
2025-08-05 01:08:03.906906: Epoch time: 431.77 s
2025-08-05 01:08:09.350736: 
2025-08-05 01:08:09.351209: Epoch 109
2025-08-05 01:08:09.360155: Current learning rate: 0.00029
2025-08-05 01:08:12.733793: 
2025-08-05 01:08:12.737190: Epoch 109
2025-08-05 01:08:12.737826: Current learning rate: 0.00029
2025-08-05 01:08:12.754117: 
2025-08-05 01:08:12.755179: Epoch 109
2025-08-05 01:08:12.755778: Current learning rate: 0.00029
2025-08-05 01:15:31.121942: train_loss 3.6296
2025-08-05 01:15:31.127054: train_loss 3.6296
2025-08-05 01:15:31.127795: train_loss 3.6296
2025-08-05 01:15:31.130469: Epoch time: 438.37 s
2025-08-05 01:15:31.130589: Epoch time: 438.39 s
2025-08-05 01:15:31.128772: Epoch time: 441.77 s
2025-08-05 01:15:35.879125: 
2025-08-05 01:15:35.881490: Epoch 110
2025-08-05 01:15:35.883792: Current learning rate: 0.00029
2025-08-05 01:15:37.125800: 
2025-08-05 01:15:37.126698: Epoch 110
2025-08-05 01:15:37.127267: Current learning rate: 0.00029
2025-08-05 01:15:37.822960: 
2025-08-05 01:15:37.838125: Epoch 110
2025-08-05 01:15:37.839066: Current learning rate: 0.00029
2025-08-05 01:22:57.509691: train_loss 3.6281
2025-08-05 01:22:57.511183: Epoch time: 441.63 s
2025-08-05 01:22:57.511712: train_loss 3.6281
2025-08-05 01:22:57.517652: Epoch time: 440.38 s
2025-08-05 01:22:57.515294: train_loss 3.6281
2025-08-05 01:22:57.526481: Epoch time: 439.69 s
2025-08-05 01:23:02.361200: 
2025-08-05 01:23:02.364417: Epoch 111
2025-08-05 01:23:02.365045: Current learning rate: 0.00029
2025-08-05 01:23:03.990874: 
2025-08-05 01:23:03.991253: Epoch 111
2025-08-05 01:23:03.991819: Current learning rate: 0.00029
2025-08-05 01:23:04.750815: 
2025-08-05 01:23:04.753264: Epoch 111
2025-08-05 01:23:04.756022: Current learning rate: 0.00029
2025-08-05 01:30:26.829804: train_loss 3.6249
2025-08-05 01:30:26.830961: Epoch time: 442.84 s
2025-08-05 01:30:26.830721: train_loss 3.6249
2025-08-05 01:30:26.832636: Epoch time: 444.47 s
2025-08-05 01:30:26.842004: train_loss 3.6249
2025-08-05 01:30:26.846239: Epoch time: 442.09 s
2025-08-05 01:30:31.101179: 
2025-08-05 01:30:31.101843: Epoch 112
2025-08-05 01:30:31.102467: Current learning rate: 0.00029
2025-08-05 01:30:32.448753: 
2025-08-05 01:30:32.450907: Epoch 112
2025-08-05 01:30:32.451628: Current learning rate: 0.00029
2025-08-05 01:30:32.913704: 
2025-08-05 01:30:32.915769: Epoch 112
2025-08-05 01:30:32.916920: Current learning rate: 0.00029
2025-08-05 01:37:51.282186: train_loss 3.6268
2025-08-05 01:37:51.282149: train_loss 3.6268
2025-08-05 01:37:51.283677: train_loss 3.6268
2025-08-05 01:37:51.289144: Epoch time: 438.83 s
2025-08-05 01:37:51.291676: Epoch time: 440.18 s
2025-08-05 01:37:51.286300: Epoch time: 438.37 s
2025-08-05 01:37:55.949147: 
2025-08-05 01:37:55.951512: Epoch 113
2025-08-05 01:37:55.952136: Current learning rate: 0.00029
2025-08-05 01:37:58.244700: 
2025-08-05 01:37:58.246830: Epoch 113
2025-08-05 01:37:58.247514: Current learning rate: 0.00029
2025-08-05 01:37:58.393767: 
2025-08-05 01:37:58.394757: Epoch 113
2025-08-05 01:37:58.395360: Current learning rate: 0.00029
2025-08-05 01:45:16.175245: train_loss 3.6272
2025-08-05 01:45:16.176739: Epoch time: 440.23 s
2025-08-05 01:45:16.184165: train_loss 3.6272
2025-08-05 01:45:16.185995: train_loss 3.6272
2025-08-05 01:45:16.187214: Epoch time: 437.79 s
2025-08-05 01:45:16.186767: Epoch time: 437.94 s
2025-08-05 01:45:20.761216: 
2025-08-05 01:45:20.761706: Epoch 114
2025-08-05 01:45:20.762370: Current learning rate: 0.00029
2025-08-05 01:45:23.249616: 
2025-08-05 01:45:23.252164: Epoch 114
2025-08-05 01:45:23.253681: Current learning rate: 0.00029
2025-08-05 01:45:23.373776: 
2025-08-05 01:45:23.375511: Epoch 114
2025-08-05 01:45:23.376180: Current learning rate: 0.00029
2025-08-05 01:52:43.051126: train_loss 3.6246
2025-08-05 01:52:43.053250: Epoch time: 442.29 s
2025-08-05 01:52:43.051366: train_loss 3.6246
2025-08-05 01:52:43.053081: train_loss 3.6246
2025-08-05 01:52:43.057586: Epoch time: 439.8 s
2025-08-05 01:52:43.055892: Epoch time: 439.68 s
2025-08-05 01:52:46.821826: 
2025-08-05 01:52:46.823284: Epoch 115
2025-08-05 01:52:46.823919: Current learning rate: 0.00029
2025-08-05 01:52:48.236310: 
2025-08-05 01:52:48.236772: Epoch 115
2025-08-05 01:52:48.238352: Current learning rate: 0.00029
2025-08-05 01:52:48.936563: 
2025-08-05 01:52:48.939543: Epoch 115
2025-08-05 01:52:48.940174: Current learning rate: 0.00029
2025-08-05 02:00:07.543150: train_loss 3.6254
2025-08-05 02:00:07.544260: Epoch time: 440.72 s
2025-08-05 02:00:07.544589: train_loss 3.6254
2025-08-05 02:00:07.552604: Epoch time: 439.31 s
2025-08-05 02:00:07.545828: train_loss 3.6254
2025-08-05 02:00:07.563992: Epoch time: 438.61 s
2025-08-05 02:00:12.645257: 
2025-08-05 02:00:12.646510: Epoch 116
2025-08-05 02:00:12.647112: Current learning rate: 0.00029
2025-08-05 02:00:14.424507: 
2025-08-05 02:00:14.427094: Epoch 116
2025-08-05 02:00:14.428848: Current learning rate: 0.00029
2025-08-05 02:00:14.598197: 
2025-08-05 02:00:14.599403: Epoch 116
2025-08-05 02:00:14.600024: Current learning rate: 0.00029
2025-08-05 02:07:37.257442: train_loss 3.6255
2025-08-05 02:07:37.263888: Epoch time: 442.83 s
2025-08-05 02:07:37.259313: train_loss 3.6255
2025-08-05 02:07:37.257654: train_loss 3.6255
2025-08-05 02:07:37.269161: Epoch time: 444.61 s
2025-08-05 02:07:37.269362: Epoch time: 442.66 s
2025-08-05 02:07:42.379425: 
2025-08-05 02:07:42.398893: Epoch 117
2025-08-05 02:07:42.401627: Current learning rate: 0.00029
2025-08-05 02:07:46.721255: 
2025-08-05 02:07:46.723758: Epoch 117
2025-08-05 02:07:46.724421: Current learning rate: 0.00029
2025-08-05 02:07:46.794970: 
2025-08-05 02:07:46.795625: Epoch 117
2025-08-05 02:07:46.796226: Current learning rate: 0.00029
2025-08-05 02:14:59.956161: train_loss 3.6265
2025-08-05 02:14:59.958012: Epoch time: 433.16 s
2025-08-05 02:14:59.957692: train_loss 3.6265
2025-08-05 02:14:59.967739: Epoch time: 433.24 s
2025-08-05 02:14:59.963842: train_loss 3.6265
2025-08-05 02:14:59.973766: Epoch time: 437.59 s
2025-08-05 02:15:04.666853: 
2025-08-05 02:15:04.669966: Epoch 118
2025-08-05 02:15:04.675329: Current learning rate: 0.00029
2025-08-05 02:15:06.482450: 
2025-08-05 02:15:06.482916: Epoch 118
2025-08-05 02:15:06.483493: Current learning rate: 0.00029
2025-08-05 02:15:06.934374: 
2025-08-05 02:15:06.936929: Epoch 118
2025-08-05 02:15:06.937649: Current learning rate: 0.00029
2025-08-05 02:22:28.988707: train_loss 3.6245
2025-08-05 02:22:28.985970: train_loss 3.6245
2025-08-05 02:22:28.993217: Epoch time: 442.5 s
2025-08-05 02:22:28.984948: train_loss 3.6245
2025-08-05 02:22:28.990300: Epoch time: 442.05 s
2025-08-05 02:22:28.995414: Epoch time: 444.32 s
2025-08-05 02:22:33.540572: 
2025-08-05 02:22:33.541364: Epoch 119
2025-08-05 02:22:33.542262: Current learning rate: 0.00029
2025-08-05 02:22:35.252722: 
2025-08-05 02:22:35.255642: Epoch 119
2025-08-05 02:22:35.257451: Current learning rate: 0.00029
2025-08-05 02:22:35.439950: 
2025-08-05 02:22:35.440998: Epoch 119
2025-08-05 02:22:35.441557: Current learning rate: 0.00029
2025-08-05 02:29:53.684826: train_loss 3.623
2025-08-05 02:29:53.686695: Epoch time: 440.15 s
2025-08-05 02:29:53.685513: train_loss 3.623
2025-08-05 02:29:53.684599: train_loss 3.623
2025-08-05 02:29:53.689543: Epoch time: 438.43 s
2025-08-05 02:29:53.689478: Epoch time: 438.25 s
2025-08-05 02:29:59.311041: 
2025-08-05 02:29:59.313755: Epoch 120
2025-08-05 02:29:59.323576: Current learning rate: 0.00029
2025-08-05 02:30:03.095624: 
2025-08-05 02:30:03.095975: Epoch 120
2025-08-05 02:30:03.096530: Current learning rate: 0.00029
2025-08-05 02:30:03.206938: 
2025-08-05 02:30:03.209504: Epoch 120
2025-08-05 02:30:03.210170: Current learning rate: 0.00029
2025-08-05 02:37:19.900046: train_loss 3.621
2025-08-05 02:37:19.900927: train_loss 3.621
2025-08-05 02:37:19.904168: Epoch time: 440.59 s
2025-08-05 02:37:19.904104: Epoch time: 436.69 s
2025-08-05 02:37:19.903997: train_loss 3.621
2025-08-05 02:37:19.908025: Epoch time: 436.8 s
2025-08-05 02:37:24.969467: 
2025-08-05 02:37:24.971363: Epoch 121
2025-08-05 02:37:24.972013: Current learning rate: 0.00029
2025-08-05 02:37:27.686085: 
2025-08-05 02:37:27.686483: Epoch 121
2025-08-05 02:37:27.687026: Current learning rate: 0.00029
2025-08-05 02:37:27.889137: 
2025-08-05 02:37:27.891351: Epoch 121
2025-08-05 02:37:27.892966: Current learning rate: 0.00029
2025-08-05 02:44:50.788859: train_loss 3.6219
2025-08-05 02:44:50.789961: train_loss 3.6219
2025-08-05 02:44:50.793622: Epoch time: 442.9 s
2025-08-05 02:44:50.790504: train_loss 3.6219
2025-08-05 02:44:50.796119: Epoch time: 443.1 s
2025-08-05 02:44:50.791130: Epoch time: 445.82 s
2025-08-05 02:44:55.843202: 
2025-08-05 02:44:55.844056: Epoch 122
2025-08-05 02:44:55.846198: Current learning rate: 0.00029
2025-08-05 02:44:57.662426: 
2025-08-05 02:44:57.664508: Epoch 122
2025-08-05 02:44:57.665385: Current learning rate: 0.00029
2025-08-05 02:44:57.747674: 
2025-08-05 02:44:57.748451: Epoch 122
2025-08-05 02:44:57.749027: Current learning rate: 0.00029
2025-08-05 02:52:22.933006: train_loss 3.6233
2025-08-05 02:52:22.937523: Epoch time: 445.18 s
2025-08-05 02:52:22.939331: train_loss 3.6233
2025-08-05 02:52:22.941762: Epoch time: 447.09 s
2025-08-05 02:52:22.945346: train_loss 3.6233
2025-08-05 02:52:22.949778: Epoch time: 445.28 s
2025-08-05 02:52:27.844872: 
2025-08-05 02:52:27.846287: Epoch 123
2025-08-05 02:52:27.846900: Current learning rate: 0.00029
2025-08-05 02:52:28.234827: 
2025-08-05 02:52:28.235286: Epoch 123
2025-08-05 02:52:28.235900: Current learning rate: 0.00029
2025-08-05 02:52:29.161163: 
2025-08-05 02:52:29.164092: Epoch 123
2025-08-05 02:52:29.164692: Current learning rate: 0.00029
2025-08-05 02:59:50.679000: train_loss 3.6236
2025-08-05 02:59:50.677055: train_loss 3.6236
2025-08-05 02:59:50.680305: Epoch time: 442.84 s
2025-08-05 02:59:50.680984: Epoch time: 441.52 s
2025-08-05 02:59:50.678992: train_loss 3.6236
2025-08-05 02:59:50.688563: Epoch time: 442.44 s
2025-08-05 02:59:55.727543: 
2025-08-05 02:59:55.741520: Epoch 124
2025-08-05 02:59:55.749872: Current learning rate: 0.00029
2025-08-05 02:59:56.275901: 
2025-08-05 02:59:56.276283: Epoch 124
2025-08-05 02:59:56.276894: Current learning rate: 0.00029
2025-08-05 03:00:04.006700: 
2025-08-05 03:00:04.008757: Epoch 124
2025-08-05 03:00:04.018654: Current learning rate: 0.00029
2025-08-05 03:07:18.442258: train_loss 3.62
2025-08-05 03:07:18.443610: Epoch time: 442.72 s
2025-08-05 03:07:18.442246: train_loss 3.62
2025-08-05 03:07:18.443020: train_loss 3.62
2025-08-05 03:07:18.446468: Epoch time: 442.17 s
2025-08-05 03:07:18.446766: Epoch time: 434.44 s
2025-08-05 03:07:22.388760: Saving checkpoint at epoch 125...
2025-08-05 03:07:22.944639: Saving checkpoint at epoch 125...
2025-08-05 03:07:23.003266: Saving checkpoint at epoch 125...
2025-08-05 03:07:26.944099: 
2025-08-05 03:07:26.944600: Epoch 125
2025-08-05 03:07:26.945225: Current learning rate: 0.00029
2025-08-05 03:07:30.816676: 
2025-08-05 03:07:30.817022: Epoch 125
2025-08-05 03:07:30.817563: Current learning rate: 0.00029
2025-08-05 03:07:30.855342: 
2025-08-05 03:07:30.857336: Epoch 125
2025-08-05 03:07:30.858010: Current learning rate: 0.00029
2025-08-05 03:14:55.714082: train_loss 3.6215
2025-08-05 03:14:55.715779: Epoch time: 448.77 s
2025-08-05 03:14:55.714525: train_loss 3.6215
2025-08-05 03:14:55.723698: Epoch time: 444.9 s
2025-08-05 03:14:55.722181: train_loss 3.6215
2025-08-05 03:14:55.726596: Epoch time: 444.87 s
2025-08-05 03:15:00.586485: 
2025-08-05 03:15:00.588866: Epoch 126
2025-08-05 03:15:00.589543: Current learning rate: 0.00029
2025-08-05 03:15:01.207944: 
2025-08-05 03:15:01.208337: Epoch 126
2025-08-05 03:15:01.208852: Current learning rate: 0.00029
2025-08-05 03:15:02.404200: 
2025-08-05 03:15:02.406351: Epoch 126
2025-08-05 03:15:02.407498: Current learning rate: 0.00029
2025-08-05 03:22:24.796792: train_loss 3.6206
2025-08-05 03:22:24.799130: Epoch time: 443.59 s
2025-08-05 03:22:24.796928: train_loss 3.6206
2025-08-05 03:22:24.803047: Epoch time: 442.39 s
2025-08-05 03:22:24.899116: train_loss 3.6206
2025-08-05 03:22:24.909153: Epoch time: 444.31 s
2025-08-05 03:22:29.037934: 
2025-08-05 03:22:29.038294: Epoch 127
2025-08-05 03:22:29.038857: Current learning rate: 0.00029
2025-08-05 03:22:35.826572: 
2025-08-05 03:22:35.828372: Epoch 127
2025-08-05 03:22:35.829177: Current learning rate: 0.00029
2025-08-05 03:22:36.577717: 
2025-08-05 03:22:36.579577: Epoch 127
2025-08-05 03:22:36.580227: Current learning rate: 0.00029
2025-08-05 03:29:46.365795: train_loss 3.6209
2025-08-05 03:29:46.367148: train_loss 3.6209
2025-08-05 03:29:46.369618: Epoch time: 437.33 s
2025-08-05 03:29:46.370531: Epoch time: 429.79 s
2025-08-05 03:29:46.367850: train_loss 3.6209
2025-08-05 03:29:46.376961: Epoch time: 430.54 s
2025-08-05 03:29:50.306714: 
2025-08-05 03:29:50.309625: Epoch 128
2025-08-05 03:29:50.315301: Current learning rate: 0.00029
2025-08-05 03:29:51.703756: 
2025-08-05 03:29:51.704449: Epoch 128
2025-08-05 03:29:51.705063: Current learning rate: 0.00029
2025-08-05 03:29:52.208104: 
2025-08-05 03:29:52.210503: Epoch 128
2025-08-05 03:29:52.212487: Current learning rate: 0.00029
2025-08-05 03:37:10.841093: train_loss 3.6228
2025-08-05 03:37:10.846734: Epoch time: 438.63 s
2025-08-05 03:37:10.846140: train_loss 3.6228
2025-08-05 03:37:10.849690: Epoch time: 439.14 s
2025-08-05 03:37:10.845083: train_loss 3.6228
2025-08-05 03:37:10.853021: Epoch time: 440.54 s
2025-08-05 03:37:14.782832: 
2025-08-05 03:37:14.785123: Epoch 129
2025-08-05 03:37:14.790410: Current learning rate: 0.00029
2025-08-05 03:37:16.139331: 
2025-08-05 03:37:16.142123: Epoch 129
2025-08-05 03:37:16.143773: Current learning rate: 0.00029
2025-08-05 03:37:17.170903: 
2025-08-05 03:37:17.171359: Epoch 129
2025-08-05 03:37:17.172014: Current learning rate: 0.00029
2025-08-05 03:44:28.483008: train_loss 3.6214
2025-08-05 03:44:28.482009: train_loss 3.6214
2025-08-05 03:44:28.484690: Epoch time: 431.31 s
2025-08-05 03:44:28.484880: Epoch time: 433.7 s
2025-08-05 03:44:28.480561: train_loss 3.6214
2025-08-05 03:44:28.493310: Epoch time: 432.34 s
2025-08-05 03:44:33.589529: 
2025-08-05 03:44:33.597330: Epoch 130
2025-08-05 03:44:33.612202: Current learning rate: 0.00029
2025-08-05 03:44:33.738270: 
2025-08-05 03:44:33.738676: Epoch 130
2025-08-05 03:44:33.739275: Current learning rate: 0.00029
2025-08-05 03:44:37.616004: 
2025-08-05 03:44:37.618404: Epoch 130
2025-08-05 03:44:37.619247: Current learning rate: 0.00029
2025-08-05 03:51:56.547806: train_loss 3.6186
2025-08-05 03:51:56.550246: Epoch time: 438.93 s
2025-08-05 03:51:56.551811: train_loss 3.6186
2025-08-05 03:51:56.555509: Epoch time: 442.81 s
2025-08-05 03:51:56.555248: train_loss 3.6186
2025-08-05 03:51:56.561717: Epoch time: 442.97 s
2025-08-05 03:52:00.515498: 
2025-08-05 03:52:00.517644: Epoch 131
2025-08-05 03:52:00.520413: Current learning rate: 0.00029
2025-08-05 03:52:03.431090: 
2025-08-05 03:52:03.433520: Epoch 131
2025-08-05 03:52:03.435282: Current learning rate: 0.00029
2025-08-05 03:52:09.879422: 
2025-08-05 03:52:09.880462: Epoch 131
2025-08-05 03:52:09.881134: Current learning rate: 0.00029
2025-08-05 03:59:24.468933: train_loss 3.621
2025-08-05 03:59:24.468996: train_loss 3.621
2025-08-05 03:59:24.471182: Epoch time: 434.59 s
2025-08-05 03:59:24.470515: Epoch time: 443.95 s
2025-08-05 03:59:24.475982: train_loss 3.621
2025-08-05 03:59:24.478992: Epoch time: 441.04 s
2025-08-05 03:59:29.577704: 
2025-08-05 03:59:29.578104: Epoch 132
2025-08-05 03:59:29.578754: Current learning rate: 0.00029
2025-08-05 03:59:32.333408: 
2025-08-05 03:59:32.335676: Epoch 132
2025-08-05 03:59:32.336341: Current learning rate: 0.00029
2025-08-05 03:59:32.437998: 
2025-08-05 03:59:32.438946: Epoch 132
2025-08-05 03:59:32.439522: Current learning rate: 0.00029
2025-08-05 04:06:43.513451: train_loss 3.6212
2025-08-05 04:06:43.516218: train_loss 3.6212
2025-08-05 04:06:43.519989: Epoch time: 431.18 s
2025-08-05 04:06:43.514598: train_loss 3.6212
2025-08-05 04:06:43.516254: Epoch time: 433.94 s
2025-08-05 04:06:43.525589: Epoch time: 431.08 s
2025-08-05 04:06:48.739286: 
2025-08-05 04:06:48.741344: Epoch 133
2025-08-05 04:06:48.742028: Current learning rate: 0.00029
2025-08-05 04:06:49.302457: 
2025-08-05 04:06:49.302848: Epoch 133
2025-08-05 04:06:49.303430: Current learning rate: 0.00029
2025-08-05 04:06:49.920946: 
2025-08-05 04:06:49.922971: Epoch 133
2025-08-05 04:06:49.924430: Current learning rate: 0.00029
2025-08-05 04:14:11.296483: train_loss 3.6175
2025-08-05 04:14:11.291686: train_loss 3.6175
2025-08-05 04:14:11.299795: Epoch time: 441.99 s
2025-08-05 04:14:11.299193: Epoch time: 442.55 s
2025-08-05 04:14:11.294962: train_loss 3.6175
2025-08-05 04:14:11.304592: Epoch time: 441.37 s
2025-08-05 04:14:15.953924: 
2025-08-05 04:14:15.956077: Epoch 134
2025-08-05 04:14:15.956802: Current learning rate: 0.00029
2025-08-05 04:14:16.985634: 
2025-08-05 04:14:16.986015: Epoch 134
2025-08-05 04:14:16.986551: Current learning rate: 0.00029
2025-08-05 04:14:17.129626: 
2025-08-05 04:14:17.131754: Epoch 134
2025-08-05 04:14:17.132490: Current learning rate: 0.00029
2025-08-05 04:21:30.106932: train_loss 3.6195
2025-08-05 04:21:30.112434: Epoch time: 432.98 s
2025-08-05 04:21:30.113012: train_loss 3.6195
2025-08-05 04:21:30.114456: Epoch time: 433.12 s
2025-08-05 04:21:30.114696: train_loss 3.6195
2025-08-05 04:21:30.117474: Epoch time: 434.16 s
2025-08-05 04:21:35.092983: 
2025-08-05 04:21:35.093424: Epoch 135
2025-08-05 04:21:35.094011: Current learning rate: 0.00029
2025-08-05 04:21:36.254389: 
2025-08-05 04:21:36.256842: Epoch 135
2025-08-05 04:21:36.257534: Current learning rate: 0.00029
2025-08-05 04:21:36.437501: 
2025-08-05 04:21:36.439499: Epoch 135
2025-08-05 04:21:36.441042: Current learning rate: 0.00029
2025-08-05 04:28:38.615406: train_loss 3.6186
2025-08-05 04:28:38.620879: Epoch time: 422.18 s
2025-08-05 04:28:38.618892: train_loss 3.6186
2025-08-05 04:28:38.623963: Epoch time: 423.53 s
2025-08-05 04:28:38.624041: train_loss 3.6186
2025-08-05 04:28:38.630221: Epoch time: 422.37 s
2025-08-05 04:28:43.574439: 
2025-08-05 04:28:43.577203: Epoch 136
2025-08-05 04:28:43.577873: Current learning rate: 0.00029
2025-08-05 04:28:44.958602: 
2025-08-05 04:28:44.960618: Epoch 136
2025-08-05 04:28:44.961286: Current learning rate: 0.00029
2025-08-05 04:28:45.228438: 
2025-08-05 04:28:45.228843: Epoch 136
2025-08-05 04:28:45.229437: Current learning rate: 0.00029
2025-08-05 04:36:08.623566: train_loss 3.6193
2025-08-05 04:36:08.626822: train_loss 3.6193
2025-08-05 04:36:08.630620: Epoch time: 445.05 s
2025-08-05 04:36:08.627039: Epoch time: 443.67 s
2025-08-05 04:36:08.630013: train_loss 3.6193
2025-08-05 04:36:08.634021: Epoch time: 443.4 s
2025-08-05 04:36:13.712668: 
2025-08-05 04:36:13.713053: Epoch 137
2025-08-05 04:36:13.713603: Current learning rate: 0.00029
2025-08-05 04:36:14.257698: 
2025-08-05 04:36:14.260974: Epoch 137
2025-08-05 04:36:14.261649: Current learning rate: 0.00029
2025-08-05 04:36:15.360315: 
2025-08-05 04:36:15.362980: Epoch 137
2025-08-05 04:36:15.363963: Current learning rate: 0.00029
2025-08-05 04:43:26.498117: train_loss 3.6168
2025-08-05 04:43:26.501638: Epoch time: 432.79 s
2025-08-05 04:43:26.499079: train_loss 3.6168
2025-08-05 04:43:26.502957: Epoch time: 431.14 s
2025-08-05 04:43:26.503471: train_loss 3.6168
2025-08-05 04:43:26.507180: Epoch time: 432.25 s
2025-08-05 04:43:31.371310: 
2025-08-05 04:43:31.372406: Epoch 138
2025-08-05 04:43:31.373031: Current learning rate: 0.00029
2025-08-05 04:43:32.679726: 
2025-08-05 04:43:32.682370: Epoch 138
2025-08-05 04:43:32.683007: Current learning rate: 0.00029
2025-08-05 04:43:32.903775: 
2025-08-05 04:43:32.906014: Epoch 138
2025-08-05 04:43:32.906615: Current learning rate: 0.00029
2025-08-05 04:50:48.770896: train_loss 3.6165
2025-08-05 04:50:48.774807: train_loss 3.6165
2025-08-05 04:50:48.775666: train_loss 3.6165
2025-08-05 04:50:48.780805: Epoch time: 435.87 s
2025-08-05 04:50:48.776042: Epoch time: 437.4 s
2025-08-05 04:50:48.780990: Epoch time: 436.1 s
2025-08-05 04:50:53.709094: 
2025-08-05 04:50:53.713140: Epoch 139
2025-08-05 04:50:53.718705: Current learning rate: 0.00029
2025-08-05 04:50:57.863873: 
2025-08-05 04:50:57.864233: Epoch 139
2025-08-05 04:50:57.865272: Current learning rate: 0.00029
2025-08-05 04:51:04.344890: 
2025-08-05 04:51:04.347151: Epoch 139
2025-08-05 04:51:04.358434: Current learning rate: 0.00029
2025-08-05 04:58:23.198611: train_loss 3.6167
2025-08-05 04:58:23.201449: Epoch time: 438.85 s
2025-08-05 04:58:23.199467: train_loss 3.6167
2025-08-05 04:58:23.204174: Epoch time: 445.34 s
2025-08-05 04:58:23.199851: train_loss 3.6167
2025-08-05 04:58:23.206093: Epoch time: 449.49 s
2025-08-05 04:58:27.675746: 
2025-08-05 04:58:27.677262: Epoch 140
2025-08-05 04:58:27.677879: Current learning rate: 0.00029
2025-08-05 04:58:28.102381: 
2025-08-05 04:58:28.105059: Epoch 140
2025-08-05 04:58:28.105764: Current learning rate: 0.00029
2025-08-05 04:58:29.393744: 
2025-08-05 04:58:29.396264: Epoch 140
2025-08-05 04:58:29.398028: Current learning rate: 0.00029
2025-08-05 05:05:52.424007: train_loss 3.6172
2025-08-05 05:05:52.425597: Epoch time: 444.75 s
2025-08-05 05:05:52.425665: train_loss 3.6172
2025-08-05 05:05:52.427205: Epoch time: 444.32 s
2025-08-05 05:05:52.429767: train_loss 3.6172
2025-08-05 05:05:52.433867: Epoch time: 443.04 s
2025-08-05 05:05:57.035150: 
2025-08-05 05:05:57.036900: Epoch 141
2025-08-05 05:05:57.037498: Current learning rate: 0.00029
2025-08-05 05:06:00.944271: 
2025-08-05 05:06:00.947136: Epoch 141
2025-08-05 05:06:00.948109: Current learning rate: 0.00029
2025-08-05 05:06:01.065298: 
2025-08-05 05:06:01.067462: Epoch 141
2025-08-05 05:06:01.068310: Current learning rate: 0.00029
2025-08-05 05:13:13.763755: train_loss 3.6175
2025-08-05 05:13:13.764891: Epoch time: 436.73 s
2025-08-05 05:13:13.762687: train_loss 3.6175
2025-08-05 05:13:13.760747: train_loss 3.6175
2025-08-05 05:13:13.766827: Epoch time: 432.82 s
2025-08-05 05:13:13.765105: Epoch time: 432.7 s
2025-08-05 05:13:18.427729: 
2025-08-05 05:13:18.428822: Epoch 142
2025-08-05 05:13:18.433356: Current learning rate: 0.00029
2025-08-05 05:13:25.096892: 
2025-08-05 05:13:25.097300: Epoch 142
2025-08-05 05:13:25.097904: Current learning rate: 0.00029
2025-08-05 05:13:28.782292: 
2025-08-05 05:13:28.784766: Epoch 142
2025-08-05 05:13:28.794951: Current learning rate: 0.00029
2025-08-05 05:20:44.003532: train_loss 3.6165
2025-08-05 05:20:44.006036: Epoch time: 438.91 s
2025-08-05 05:20:44.004434: train_loss 3.6165
2025-08-05 05:20:44.007197: Epoch time: 435.22 s
2025-08-05 05:20:44.006725: train_loss 3.6165
2025-08-05 05:20:44.014338: Epoch time: 445.58 s
2025-08-05 05:20:48.579792: 
2025-08-05 05:20:48.582253: Epoch 143
2025-08-05 05:20:48.582948: Current learning rate: 0.00029
2025-08-05 05:20:48.800089: 
2025-08-05 05:20:48.801066: Epoch 143
2025-08-05 05:20:48.802002: Current learning rate: 0.00029
2025-08-05 05:20:50.166938: 
2025-08-05 05:20:50.169363: Epoch 143
2025-08-05 05:20:50.171671: Current learning rate: 0.00029
2025-08-05 05:28:13.395694: train_loss 3.6167
2025-08-05 05:28:13.397563: Epoch time: 444.6 s
2025-08-05 05:28:13.392529: train_loss 3.6167
2025-08-05 05:28:13.398693: train_loss 3.6167
2025-08-05 05:28:13.404394: Epoch time: 443.23 s
2025-08-05 05:28:13.401905: Epoch time: 444.81 s
2025-08-05 05:28:17.562111: 
2025-08-05 05:28:17.565111: Epoch 144
2025-08-05 05:28:17.566285: Current learning rate: 0.00028
2025-08-05 05:28:20.264736: 
2025-08-05 05:28:20.267247: Epoch 144
2025-08-05 05:28:20.268873: Current learning rate: 0.00028
2025-08-05 05:28:20.413227: 
2025-08-05 05:28:20.414194: Epoch 144
2025-08-05 05:28:20.414768: Current learning rate: 0.00028
2025-08-05 05:35:34.716825: train_loss 3.6158
2025-08-05 05:35:34.718561: Epoch time: 434.45 s
2025-08-05 05:35:34.717455: train_loss 3.6158
2025-08-05 05:35:34.718251: train_loss 3.6158
2025-08-05 05:35:34.723620: Epoch time: 434.31 s
2025-08-05 05:35:34.723379: Epoch time: 437.16 s
2025-08-05 05:35:42.579630: 
2025-08-05 05:35:42.582436: Epoch 145
2025-08-05 05:35:42.592077: Current learning rate: 0.00028
2025-08-05 05:35:43.661011: 
2025-08-05 05:35:43.661352: Epoch 145
2025-08-05 05:35:43.661918: Current learning rate: 0.00028
2025-08-05 05:35:43.821678: 
2025-08-05 05:35:43.824072: Epoch 145
2025-08-05 05:35:43.824723: Current learning rate: 0.00028
2025-08-05 05:42:57.381035: train_loss 3.6173
2025-08-05 05:42:57.385720: Epoch time: 433.56 s
2025-08-05 05:42:57.379571: train_loss 3.6173
2025-08-05 05:42:57.389506: Epoch time: 433.72 s
2025-08-05 05:42:57.385646: train_loss 3.6173
2025-08-05 05:42:57.394729: Epoch time: 434.81 s
2025-08-05 05:43:01.224744: 
2025-08-05 05:43:01.225150: Epoch 146
2025-08-05 05:43:01.225704: Current learning rate: 0.00028
2025-08-05 05:43:03.519012: 
2025-08-05 05:43:03.519444: Epoch 146
2025-08-05 05:43:03.520120: Current learning rate: 0.00028
2025-08-05 05:43:03.643919: 
2025-08-05 05:43:03.646522: Epoch 146
2025-08-05 05:43:03.647178: Current learning rate: 0.00028
2025-08-05 05:50:22.454527: train_loss 3.615
2025-08-05 05:50:22.459721: Epoch time: 438.81 s
2025-08-05 05:50:22.456824: train_loss 3.615
2025-08-05 05:50:22.462272: Epoch time: 441.23 s
2025-08-05 05:50:22.457924: train_loss 3.615
2025-08-05 05:50:22.464246: Epoch time: 438.94 s
2025-08-05 05:50:27.332430: 
2025-08-05 05:50:27.335338: Epoch 147
2025-08-05 05:50:27.336235: Current learning rate: 0.00028
2025-08-05 05:50:29.114470: 
2025-08-05 05:50:29.114863: Epoch 147
2025-08-05 05:50:29.115489: Current learning rate: 0.00028
2025-08-05 05:50:29.246382: 
2025-08-05 05:50:29.248855: Epoch 147
2025-08-05 05:50:29.249565: Current learning rate: 0.00028
2025-08-05 05:57:48.629490: train_loss 3.6159
2025-08-05 05:57:48.630575: Epoch time: 439.52 s
2025-08-05 05:57:48.629911: train_loss 3.6159
2025-08-05 05:57:48.627447: train_loss 3.6159
2025-08-05 05:57:48.632885: Epoch time: 439.38 s
2025-08-05 05:57:48.632114: Epoch time: 441.3 s
2025-08-05 05:57:54.087952: 
2025-08-05 05:57:54.090201: Epoch 148
2025-08-05 05:57:54.095586: Current learning rate: 0.00028
2025-08-05 05:57:55.096354: 
2025-08-05 05:57:55.097402: Epoch 148
2025-08-05 05:57:55.098602: Current learning rate: 0.00028
2025-08-05 05:57:55.192792: 
2025-08-05 05:57:55.195505: Epoch 148
2025-08-05 05:57:55.196292: Current learning rate: 0.00028
2025-08-05 06:05:11.636080: train_loss 3.6146
2025-08-05 06:05:11.637426: Epoch time: 436.54 s
2025-08-05 06:05:11.637255: train_loss 3.6146
2025-08-05 06:05:11.638490: Epoch time: 436.45 s
2025-08-05 06:05:11.638985: train_loss 3.6146
2025-08-05 06:05:11.644639: Epoch time: 437.55 s
2025-08-05 06:05:15.542638: 
2025-08-05 06:05:15.543663: Epoch 149
2025-08-05 06:05:15.544676: Current learning rate: 0.00028
2025-08-05 06:05:16.406682: 
2025-08-05 06:05:16.408913: Epoch 149
2025-08-05 06:05:16.410635: Current learning rate: 0.00028
2025-08-05 06:05:19.330765: 
2025-08-05 06:05:19.333250: Epoch 149
2025-08-05 06:05:19.334317: Current learning rate: 0.00028
2025-08-05 06:12:39.144731: train_loss 3.6145
2025-08-05 06:12:39.146128: Epoch time: 439.81 s
2025-08-05 06:12:39.144459: train_loss 3.6145
2025-08-05 06:12:39.148003: Epoch time: 443.6 s
2025-08-05 06:12:39.150447: train_loss 3.6145
2025-08-05 06:12:39.161114: Epoch time: 442.74 s
2025-08-05 06:12:42.520352: Saving checkpoint at epoch 150...
2025-08-05 06:12:44.378886: Saving checkpoint at epoch 150...
2025-08-05 06:12:44.622917: Saving checkpoint at epoch 150...
2025-08-05 06:12:46.736830: 
2025-08-05 06:12:46.738983: Epoch 150
2025-08-05 06:12:46.739592: Current learning rate: 0.00028
2025-08-05 06:12:48.970309: 
2025-08-05 06:12:48.971041: Epoch 150
2025-08-05 06:12:48.971547: Current learning rate: 0.00028
2025-08-05 06:12:49.051302: 
2025-08-05 06:12:49.053592: Epoch 150
2025-08-05 06:12:49.054345: Current learning rate: 0.00028
2025-08-05 06:20:00.750843: train_loss 3.6142
2025-08-05 06:20:00.753671: train_loss 3.6142
2025-08-05 06:20:00.752870: Epoch time: 431.7 s
2025-08-05 06:20:00.755223: train_loss 3.6142
2025-08-05 06:20:00.760027: Epoch time: 431.79 s
2025-08-05 06:20:00.758077: Epoch time: 434.02 s
2025-08-05 06:20:06.115577: 
2025-08-05 06:20:06.116727: Epoch 151
2025-08-05 06:20:06.117324: Current learning rate: 0.00028
2025-08-05 06:20:08.551626: 
2025-08-05 06:20:08.554026: Epoch 151
2025-08-05 06:20:08.555387: Current learning rate: 0.00028
2025-08-05 06:20:08.717854: 
2025-08-05 06:20:08.718772: Epoch 151
2025-08-05 06:20:08.719321: Current learning rate: 0.00028
2025-08-05 06:27:37.026026: train_loss 3.6152
2025-08-05 06:27:37.027245: Epoch time: 448.3 s
2025-08-05 06:27:37.025846: train_loss 3.6152
2025-08-05 06:27:37.023880: train_loss 3.6152
2025-08-05 06:27:37.029460: Epoch time: 448.47 s
2025-08-05 06:27:37.027889: Epoch time: 450.91 s
2025-08-05 06:27:40.939255: 
2025-08-05 06:27:40.940413: Epoch 152
2025-08-05 06:27:40.944556: Current learning rate: 0.00028
2025-08-05 06:27:44.159822: 
2025-08-05 06:27:44.162216: Epoch 152
2025-08-05 06:27:44.164205: Current learning rate: 0.00028
2025-08-05 06:27:44.310551: 
2025-08-05 06:27:44.310910: Epoch 152
2025-08-05 06:27:44.311554: Current learning rate: 0.00028
2025-08-05 06:35:01.266202: train_loss 3.6176
2025-08-05 06:35:01.258581: train_loss 3.6176
2025-08-05 06:35:01.267884: Epoch time: 437.11 s
2025-08-05 06:35:01.269451: Epoch time: 440.32 s
2025-08-05 06:35:01.262933: train_loss 3.6176
2025-08-05 06:35:01.273518: Epoch time: 436.95 s
2025-08-05 06:35:06.377031: 
2025-08-05 06:35:06.378036: Epoch 153
2025-08-05 06:35:06.378700: Current learning rate: 0.00028
2025-08-05 06:35:07.081052: 
2025-08-05 06:35:07.081450: Epoch 153
2025-08-05 06:35:07.082168: Current learning rate: 0.00028
2025-08-05 06:35:07.785836: 
2025-08-05 06:35:07.788682: Epoch 153
2025-08-05 06:35:07.790327: Current learning rate: 0.00028
2025-08-05 06:42:22.960592: train_loss 3.6162
2025-08-05 06:42:22.965173: Epoch time: 435.88 s
2025-08-05 06:42:22.957948: train_loss 3.6162
2025-08-05 06:42:22.966612: Epoch time: 436.58 s
2025-08-05 06:42:22.960483: train_loss 3.6162
2025-08-05 06:42:22.968022: Epoch time: 435.18 s
2025-08-05 06:42:27.860398: 
2025-08-05 06:42:27.862754: Epoch 154
2025-08-05 06:42:27.863437: Current learning rate: 0.00028
2025-08-05 06:42:29.700433: 
2025-08-05 06:42:29.701001: Epoch 154
2025-08-05 06:42:29.701507: Current learning rate: 0.00028
2025-08-05 06:42:29.899597: 
2025-08-05 06:42:29.901768: Epoch 154
2025-08-05 06:42:29.903288: Current learning rate: 0.00028
2025-08-05 06:49:56.387030: train_loss 3.6117
2025-08-05 06:49:56.389283: train_loss 3.6117
2025-08-05 06:49:56.390418: Epoch time: 448.53 s
2025-08-05 06:49:56.391648: Epoch time: 446.49 s
2025-08-05 06:49:56.388090: train_loss 3.6117
2025-08-05 06:49:56.398179: Epoch time: 446.69 s
2025-08-05 06:50:00.994797: 
2025-08-05 06:50:00.995196: Epoch 155
2025-08-05 06:50:00.995783: Current learning rate: 0.00028
2025-08-05 06:50:02.577368: 
2025-08-05 06:50:02.577795: Epoch 155
2025-08-05 06:50:02.579007: Current learning rate: 0.00028
2025-08-05 06:50:02.831739: 
2025-08-05 06:50:02.834506: Epoch 155
2025-08-05 06:50:02.835267: Current learning rate: 0.00028
2025-08-05 06:57:17.738563: train_loss 3.613
2025-08-05 06:57:17.735672: train_loss 3.613
2025-08-05 06:57:17.740004: Epoch time: 435.16 s
2025-08-05 06:57:17.741579: Epoch time: 434.9 s
2025-08-05 06:57:17.733625: train_loss 3.613
2025-08-05 06:57:17.750770: Epoch time: 436.74 s
2025-08-05 06:57:23.531142: 
2025-08-05 06:57:23.531599: Epoch 156
2025-08-05 06:57:23.536972: Current learning rate: 0.00028
2025-08-05 06:57:24.136841: 
2025-08-05 06:57:24.137245: Epoch 156
2025-08-05 06:57:24.137816: Current learning rate: 0.00028
2025-08-05 06:57:24.794021: 
2025-08-05 06:57:24.815281: Epoch 156
2025-08-05 06:57:24.817148: Current learning rate: 0.00028
2025-08-05 07:04:38.311192: train_loss 3.6127
2025-08-05 07:04:38.313335: train_loss 3.6127
2025-08-05 07:04:38.307304: train_loss 3.6127
2025-08-05 07:04:38.314630: Epoch time: 434.18 s
2025-08-05 07:04:38.312809: Epoch time: 433.52 s
2025-08-05 07:04:38.314615: Epoch time: 434.78 s
2025-08-05 07:04:43.080526: 
2025-08-05 07:04:43.081699: Epoch 157
2025-08-05 07:04:43.083442: Current learning rate: 0.00028
2025-08-05 07:04:47.836997: 
2025-08-05 07:04:47.837396: Epoch 157
2025-08-05 07:04:47.838391: Current learning rate: 0.00028
2025-08-05 07:04:47.894015: 
2025-08-05 07:04:47.896591: Epoch 157
2025-08-05 07:04:47.898843: Current learning rate: 0.00028
2025-08-05 07:12:11.282113: train_loss 3.6135
2025-08-05 07:12:11.288827: Epoch time: 448.2 s
2025-08-05 07:12:11.287772: train_loss 3.6135
2025-08-05 07:12:11.294139: Epoch time: 443.39 s
2025-08-05 07:12:11.282044: train_loss 3.6135
2025-08-05 07:12:11.296137: Epoch time: 443.44 s
2025-08-05 07:12:15.813962: 
2025-08-05 07:12:15.814919: Epoch 158
2025-08-05 07:12:15.815554: Current learning rate: 0.00028
2025-08-05 07:12:17.400236: 
2025-08-05 07:12:17.400676: Epoch 158
2025-08-05 07:12:17.401332: Current learning rate: 0.00028
2025-08-05 07:12:17.545766: 
2025-08-05 07:12:17.548288: Epoch 158
2025-08-05 07:12:17.548906: Current learning rate: 0.00028
2025-08-05 07:19:30.535992: train_loss 3.6126
2025-08-05 07:19:30.539566: Epoch time: 434.72 s
2025-08-05 07:19:30.534770: train_loss 3.6126
2025-08-05 07:19:30.535524: train_loss 3.6126
2025-08-05 07:19:30.542465: Epoch time: 432.99 s
2025-08-05 07:19:30.540127: Epoch time: 433.14 s
2025-08-05 07:19:36.502800: 
2025-08-05 07:19:36.505434: Epoch 159
2025-08-05 07:19:36.513710: Current learning rate: 0.00028
2025-08-05 07:19:39.080369: 
2025-08-05 07:19:39.083091: Epoch 159
2025-08-05 07:19:39.085787: Current learning rate: 0.00028
2025-08-05 07:19:39.158860: 
2025-08-05 07:19:39.159240: Epoch 159
2025-08-05 07:19:39.159816: Current learning rate: 0.00028
2025-08-05 07:27:02.456136: train_loss 3.6126
2025-08-05 07:27:02.457355: train_loss 3.6126
2025-08-05 07:27:02.458413: Epoch time: 443.3 s
2025-08-05 07:27:02.457541: Epoch time: 445.95 s
2025-08-05 07:27:02.462246: train_loss 3.6126
2025-08-05 07:27:02.465301: Epoch time: 443.37 s
2025-08-05 07:27:07.538801: 
2025-08-05 07:27:07.539286: Epoch 160
2025-08-05 07:27:07.540021: Current learning rate: 0.00028
2025-08-05 07:27:08.064340: 
2025-08-05 07:27:08.067024: Epoch 160
2025-08-05 07:27:08.067708: Current learning rate: 0.00028
2025-08-05 07:27:08.775062: 
2025-08-05 07:27:08.775461: Epoch 160
2025-08-05 07:27:08.776028: Current learning rate: 0.00028
2025-08-05 07:34:29.684639: train_loss 3.6133
2025-08-05 07:34:29.684326: train_loss 3.6133
2025-08-05 07:34:29.684541: train_loss 3.6133
2025-08-05 07:34:29.689978: Epoch time: 442.15 s
2025-08-05 07:34:29.689718: Epoch time: 440.91 s
2025-08-05 07:34:29.687180: Epoch time: 441.62 s
2025-08-05 07:34:33.545032: 
2025-08-05 07:34:33.545500: Epoch 161
2025-08-05 07:34:33.546102: Current learning rate: 0.00028
2025-08-05 07:34:35.751712: 
2025-08-05 07:34:35.754213: Epoch 161
2025-08-05 07:34:35.754956: Current learning rate: 0.00028
2025-08-05 07:34:35.964950: 
2025-08-05 07:34:35.967113: Epoch 161
2025-08-05 07:34:35.967735: Current learning rate: 0.00028
2025-08-05 07:41:50.060621: train_loss 3.6137
2025-08-05 07:41:50.064058: Epoch time: 434.1 s
2025-08-05 07:41:50.064129: train_loss 3.6137
2025-08-05 07:41:50.069651: Epoch time: 436.52 s
2025-08-05 07:41:50.061932: train_loss 3.6137
2025-08-05 07:41:50.077174: Epoch time: 434.31 s
2025-08-05 07:41:55.434856: 
2025-08-05 07:41:55.438087: Epoch 162
2025-08-05 07:41:55.447044: Current learning rate: 0.00028
2025-08-05 07:42:00.029756: 
2025-08-05 07:42:00.030788: Epoch 162
2025-08-05 07:42:00.031408: Current learning rate: 0.00028
2025-08-05 07:42:00.034894: 
2025-08-05 07:42:00.037531: Epoch 162
2025-08-05 07:42:00.039355: Current learning rate: 0.00028
2025-08-05 07:49:18.422959: train_loss 3.6125
2025-08-05 07:49:18.426846: Epoch time: 438.39 s
2025-08-05 07:49:18.429342: train_loss 3.6125
2025-08-05 07:49:18.432139: Epoch time: 443.0 s
2025-08-05 07:49:18.431021: train_loss 3.6125
2025-08-05 07:49:18.434072: Epoch time: 438.4 s
2025-08-05 07:49:23.765090: 
2025-08-05 07:49:23.766266: Epoch 163
2025-08-05 07:49:23.766858: Current learning rate: 0.00028
2025-08-05 07:49:24.917543: 
2025-08-05 07:49:24.918258: Epoch 163
2025-08-05 07:49:24.918817: Current learning rate: 0.00028
2025-08-05 07:49:25.147924: 
2025-08-05 07:49:25.150837: Epoch 163
2025-08-05 07:49:25.151552: Current learning rate: 0.00028
2025-08-05 07:56:53.500868: train_loss 3.6127
2025-08-05 07:56:53.502741: Epoch time: 449.74 s
2025-08-05 07:56:53.508621: train_loss 3.6127
2025-08-05 07:56:53.511615: Epoch time: 448.36 s
2025-08-05 07:56:53.511480: train_loss 3.6127
2025-08-05 07:56:53.514606: Epoch time: 448.59 s
2025-08-05 07:56:58.824225: 
2025-08-05 07:56:58.825200: Epoch 164
2025-08-05 07:56:58.825812: Current learning rate: 0.00028
2025-08-05 07:56:59.852686: 
2025-08-05 07:56:59.853041: Epoch 164
2025-08-05 07:56:59.853619: Current learning rate: 0.00028
2025-08-05 07:57:00.114578: 
2025-08-05 07:57:00.117035: Epoch 164
2025-08-05 07:57:00.117687: Current learning rate: 0.00028
2025-08-05 08:04:17.429062: train_loss 3.6124
2025-08-05 08:04:17.437111: Epoch time: 437.58 s
2025-08-05 08:04:17.436850: train_loss 3.6124
2025-08-05 08:04:17.442072: Epoch time: 437.32 s
2025-08-05 08:04:17.435021: train_loss 3.6124
2025-08-05 08:04:17.452461: Epoch time: 438.61 s
2025-08-05 08:04:22.199238: 
2025-08-05 08:04:22.199692: Epoch 165
2025-08-05 08:04:22.200906: Current learning rate: 0.00028
2025-08-05 08:04:22.396708: 
2025-08-05 08:04:22.399823: Epoch 165
2025-08-05 08:04:22.400437: Current learning rate: 0.00028
2025-08-05 08:04:23.715734: 
2025-08-05 08:04:23.717037: Epoch 165
2025-08-05 08:04:23.717600: Current learning rate: 0.00028
2025-08-05 08:11:48.284842: train_loss 3.6118
2025-08-05 08:11:48.286269: train_loss 3.6118
2025-08-05 08:11:48.290033: Epoch time: 446.09 s
2025-08-05 08:11:48.287716: train_loss 3.6118
2025-08-05 08:11:48.290760: Epoch time: 444.57 s
2025-08-05 08:11:48.287653: Epoch time: 445.89 s
2025-08-05 08:11:53.313657: 
2025-08-05 08:11:53.316248: Epoch 166
2025-08-05 08:11:53.320941: Current learning rate: 0.00028
2025-08-05 08:11:54.595531: 
2025-08-05 08:11:54.596048: Epoch 166
2025-08-05 08:11:54.596571: Current learning rate: 0.00028
2025-08-05 08:11:54.810844: 
2025-08-05 08:11:54.813810: Epoch 166
2025-08-05 08:11:54.815588: Current learning rate: 0.00028
2025-08-05 08:19:13.348912: train_loss 3.6113
2025-08-05 08:19:13.353180: train_loss 3.6113
2025-08-05 08:19:13.355489: Epoch time: 438.54 s
2025-08-05 08:19:13.354371: Epoch time: 440.04 s
2025-08-05 08:19:13.352309: train_loss 3.6113
2025-08-05 08:19:13.369290: Epoch time: 438.76 s
2025-08-05 08:19:18.577031: 
2025-08-05 08:19:18.577407: Epoch 167
2025-08-05 08:19:18.577992: Current learning rate: 0.00028
2025-08-05 08:19:19.174520: 
2025-08-05 08:19:19.177582: Epoch 167
2025-08-05 08:19:19.178452: Current learning rate: 0.00028
2025-08-05 08:19:21.390434: 
2025-08-05 08:19:21.392614: Epoch 167
2025-08-05 08:19:21.393348: Current learning rate: 0.00028
2025-08-05 08:26:38.565360: train_loss 3.6118
2025-08-05 08:26:38.567776: Epoch time: 439.39 s
2025-08-05 08:26:38.569968: train_loss 3.6118
2025-08-05 08:26:38.572493: Epoch time: 437.18 s
2025-08-05 08:26:38.568009: train_loss 3.6118
2025-08-05 08:26:38.575641: Epoch time: 439.99 s
2025-08-05 08:26:43.313014: 
2025-08-05 08:26:43.316219: Epoch 168
2025-08-05 08:26:43.317801: Current learning rate: 0.00028
2025-08-05 08:26:45.734143: 
2025-08-05 08:26:45.736537: Epoch 168
2025-08-05 08:26:45.739660: Current learning rate: 0.00028
2025-08-05 08:26:46.157480: 
2025-08-05 08:26:46.158085: Epoch 168
2025-08-05 08:26:46.158697: Current learning rate: 0.00028
2025-08-05 08:34:07.029968: train_loss 3.6115
2025-08-05 08:34:07.032840: Epoch time: 441.3 s
2025-08-05 08:34:07.040082: train_loss 3.6115
2025-08-05 08:34:07.042490: Epoch time: 440.88 s
2025-08-05 08:34:07.048011: train_loss 3.6115
2025-08-05 08:34:07.059507: Epoch time: 443.74 s
2025-08-05 08:34:11.774338: 
2025-08-05 08:34:11.775189: Epoch 169
2025-08-05 08:34:11.775752: Current learning rate: 0.00028
2025-08-05 08:34:12.963283: 
2025-08-05 08:34:12.965741: Epoch 169
2025-08-05 08:34:12.966590: Current learning rate: 0.00028
2025-08-05 08:34:13.789957: 
2025-08-05 08:34:13.791809: Epoch 169
2025-08-05 08:34:13.792393: Current learning rate: 0.00028
2025-08-05 08:41:33.144668: train_loss 3.6118
2025-08-05 08:41:33.147565: Epoch time: 439.36 s
2025-08-05 08:41:33.147094: train_loss 3.6118
2025-08-05 08:41:33.153718: Epoch time: 441.37 s
2025-08-05 08:41:33.151959: train_loss 3.6118
2025-08-05 08:41:33.157581: Epoch time: 440.19 s
2025-08-05 08:41:38.814712: 
2025-08-05 08:41:38.817017: Epoch 170
2025-08-05 08:41:38.823978: Current learning rate: 0.00028
2025-08-05 08:41:41.579480: 
2025-08-05 08:41:41.581715: Epoch 170
2025-08-05 08:41:41.583348: Current learning rate: 0.00028
2025-08-05 08:41:41.716236: 
2025-08-05 08:41:41.716594: Epoch 170
2025-08-05 08:41:41.717170: Current learning rate: 0.00028
2025-08-05 08:48:57.661620: train_loss 3.6096
2025-08-05 08:48:57.663808: train_loss 3.6096
2025-08-05 08:48:57.670229: Epoch time: 435.95 s
2025-08-05 08:48:57.668019: Epoch time: 438.85 s
2025-08-05 08:48:57.679185: train_loss 3.6096
2025-08-05 08:48:57.682430: Epoch time: 436.1 s
2025-08-05 08:49:01.786420: 
2025-08-05 08:49:01.789899: Epoch 171
2025-08-05 08:49:01.797970: Current learning rate: 0.00028
2025-08-05 08:49:02.225973: 
2025-08-05 08:49:02.227869: Epoch 171
2025-08-05 08:49:02.228469: Current learning rate: 0.00028
2025-08-05 08:49:10.578350: 
2025-08-05 08:49:10.580647: Epoch 171
2025-08-05 08:49:10.587990: Current learning rate: 0.00028
2025-08-05 08:56:28.618894: train_loss 3.6113
2025-08-05 08:56:28.622771: Epoch time: 446.39 s
2025-08-05 08:56:28.620481: train_loss 3.6113
2025-08-05 08:56:28.624822: Epoch time: 438.04 s
2025-08-05 08:56:28.695117: train_loss 3.6113
2025-08-05 08:56:28.700075: Epoch time: 446.91 s
2025-08-05 08:56:34.893532: 
2025-08-05 08:56:34.893974: Epoch 172
2025-08-05 08:56:34.894638: Current learning rate: 0.00028
2025-08-05 08:56:36.389907: 
2025-08-05 08:56:36.392223: Epoch 172
2025-08-05 08:56:36.393189: Current learning rate: 0.00028
2025-08-05 08:56:39.473603: 
2025-08-05 08:56:39.475748: Epoch 172
2025-08-05 08:56:39.476458: Current learning rate: 0.00028
2025-08-05 09:03:51.698473: train_loss 3.6115
2025-08-05 09:03:51.700825: train_loss 3.6115
2025-08-05 09:03:51.703584: Epoch time: 436.81 s
2025-08-05 09:03:51.702481: Epoch time: 432.23 s
2025-08-05 09:03:51.702707: train_loss 3.6115
2025-08-05 09:03:51.709741: Epoch time: 435.31 s
2025-08-05 09:03:58.105252: 
2025-08-05 09:03:58.108426: Epoch 173
2025-08-05 09:03:58.117340: Current learning rate: 0.00028
2025-08-05 09:04:01.971048: 
2025-08-05 09:04:01.972107: Epoch 173
2025-08-05 09:04:01.972695: Current learning rate: 0.00028
2025-08-05 09:04:02.720224: 
2025-08-05 09:04:02.722450: Epoch 173
2025-08-05 09:04:02.725756: Current learning rate: 0.00028
2025-08-05 09:11:14.165063: train_loss 3.6104
2025-08-05 09:11:14.166426: Epoch time: 436.06 s
2025-08-05 09:11:14.168632: train_loss 3.6104
2025-08-05 09:11:14.178536: Epoch time: 432.19 s
2025-08-05 09:11:14.175373: train_loss 3.6104
2025-08-05 09:11:14.181829: Epoch time: 431.46 s
2025-08-05 09:11:19.493536: 
2025-08-05 09:11:19.493923: Epoch 174
2025-08-05 09:11:19.494512: Current learning rate: 0.00028
2025-08-05 09:11:20.602049: 
2025-08-05 09:11:20.610677: Epoch 174
2025-08-05 09:11:20.621001: Current learning rate: 0.00028
2025-08-05 09:11:23.837548: 
2025-08-05 09:11:23.839519: Epoch 174
2025-08-05 09:11:23.840291: Current learning rate: 0.00028
2025-08-05 09:18:48.377675: train_loss 3.6123
2025-08-05 09:18:48.379850: Epoch time: 448.89 s
2025-08-05 09:18:48.379067: train_loss 3.6123
2025-08-05 09:18:48.382089: Epoch time: 447.78 s
2025-08-05 09:18:48.379275: train_loss 3.6123
2025-08-05 09:18:48.384132: Epoch time: 444.54 s
2025-08-05 09:18:53.187950: Saving checkpoint at epoch 175...
2025-08-05 09:18:54.994773: Saving checkpoint at epoch 175...
2025-08-05 09:18:57.117890: 
2025-08-05 09:18:57.118268: Epoch 175
2025-08-05 09:18:57.118849: Current learning rate: 0.00028
2025-08-05 09:18:57.176821: Saving checkpoint at epoch 175...
2025-08-05 09:18:58.957126: 
2025-08-05 09:18:58.960557: Epoch 175
2025-08-05 09:18:58.961255: Current learning rate: 0.00028
2025-08-05 09:19:01.193410: 
2025-08-05 09:19:01.196694: Epoch 175
2025-08-05 09:19:01.197559: Current learning rate: 0.00028
2025-08-05 09:26:23.003510: train_loss 3.6094
2025-08-05 09:26:23.010920: Epoch time: 445.89 s
2025-08-05 09:26:23.006604: train_loss 3.6094
2025-08-05 09:26:23.012210: Epoch time: 441.81 s
2025-08-05 09:26:23.007974: train_loss 3.6094
2025-08-05 09:26:23.017990: Epoch time: 444.04 s
2025-08-05 09:26:28.453656: 
2025-08-05 09:26:28.455312: Epoch 176
2025-08-05 09:26:28.455946: Current learning rate: 0.00028
2025-08-05 09:26:30.798016: 
2025-08-05 09:26:30.800834: Epoch 176
2025-08-05 09:26:30.802370: Current learning rate: 0.00028
2025-08-05 09:26:30.878779: 
2025-08-05 09:26:30.880846: Epoch 176
2025-08-05 09:26:30.882034: Current learning rate: 0.00028
2025-08-05 09:33:56.199342: train_loss 3.6102
2025-08-05 09:33:56.199363: train_loss 3.6102
2025-08-05 09:33:56.198881: train_loss 3.6102
2025-08-05 09:33:56.201340: Epoch time: 445.32 s
2025-08-05 09:33:56.206154: Epoch time: 447.75 s
2025-08-05 09:33:56.205835: Epoch time: 445.4 s
2025-08-05 09:34:02.085644: 
2025-08-05 09:34:02.086068: Epoch 177
2025-08-05 09:34:02.086679: Current learning rate: 0.00028
2025-08-05 09:34:03.120511: 
2025-08-05 09:34:03.122649: Epoch 177
2025-08-05 09:34:03.123311: Current learning rate: 0.00028
2025-08-05 09:34:03.243096: 
2025-08-05 09:34:03.244221: Epoch 177
2025-08-05 09:34:03.244783: Current learning rate: 0.00028
2025-08-05 09:41:24.057285: train_loss 3.6097
2025-08-05 09:41:24.058427: Epoch time: 440.82 s
2025-08-05 09:41:24.056469: train_loss 3.6097
2025-08-05 09:41:24.057139: train_loss 3.6097
2025-08-05 09:41:24.059738: Epoch time: 441.97 s
2025-08-05 09:41:24.064260: Epoch time: 440.94 s
2025-08-05 09:41:27.912942: 
2025-08-05 09:41:27.913316: Epoch 178
2025-08-05 09:41:27.913881: Current learning rate: 0.00028
2025-08-05 09:41:29.090903: 
2025-08-05 09:41:29.091453: Epoch 178
2025-08-05 09:41:29.092462: Current learning rate: 0.00028
2025-08-05 09:41:29.235551: 
2025-08-05 09:41:29.237823: Epoch 178
2025-08-05 09:41:29.238616: Current learning rate: 0.00028
2025-08-05 09:48:49.396579: train_loss 3.61
2025-08-05 09:48:49.398977: train_loss 3.61
2025-08-05 09:48:49.402130: Epoch time: 441.49 s
2025-08-05 09:48:49.400854: Epoch time: 440.16 s
2025-08-05 09:48:49.398301: train_loss 3.61
2025-08-05 09:48:49.406175: Epoch time: 440.31 s
2025-08-05 09:48:54.575429: 
2025-08-05 09:48:54.579032: Epoch 179
2025-08-05 09:48:54.579805: Current learning rate: 0.00028
2025-08-05 09:48:56.514334: 
2025-08-05 09:48:56.515630: Epoch 179
2025-08-05 09:48:56.516230: Current learning rate: 0.00028
2025-08-05 09:48:56.652986: 
2025-08-05 09:48:56.654947: Epoch 179
2025-08-05 09:48:56.657436: Current learning rate: 0.00028
2025-08-05 09:56:16.254458: train_loss 3.6094
2025-08-05 09:56:16.256545: Epoch time: 441.68 s
2025-08-05 09:56:16.257370: train_loss 3.6094
2025-08-05 09:56:16.262562: Epoch time: 439.74 s
2025-08-05 09:56:16.259960: train_loss 3.6094
2025-08-05 09:56:16.266854: Epoch time: 439.61 s
2025-08-05 09:56:20.931517: 
2025-08-05 09:56:20.934536: Epoch 180
2025-08-05 09:56:20.936471: Current learning rate: 0.00028
2025-08-05 09:56:22.178324: 
2025-08-05 09:56:22.180405: Epoch 180
2025-08-05 09:56:22.182019: Current learning rate: 0.00028
2025-08-05 09:56:22.395196: 
2025-08-05 09:56:22.396007: Epoch 180
2025-08-05 09:56:22.396574: Current learning rate: 0.00028
2025-08-05 10:03:40.573175: train_loss 3.6094
2025-08-05 10:03:40.580294: train_loss 3.6094
2025-08-05 10:03:40.581483: Epoch time: 438.19 s
2025-08-05 10:03:40.575863: train_loss 3.6094
2025-08-05 10:03:40.582356: Epoch time: 438.4 s
2025-08-05 10:03:40.579589: Epoch time: 439.64 s
2025-08-05 10:03:45.405300: 
2025-08-05 10:03:45.405707: Epoch 181
2025-08-05 10:03:45.406257: Current learning rate: 0.00028
2025-08-05 10:03:47.773913: 
2025-08-05 10:03:47.776227: Epoch 181
2025-08-05 10:03:47.777948: Current learning rate: 0.00028
2025-08-05 10:03:47.922509: 
2025-08-05 10:03:47.925155: Epoch 181
2025-08-05 10:03:47.925995: Current learning rate: 0.00028
2025-08-05 10:11:02.833701: train_loss 3.6096
2025-08-05 10:11:02.841527: Epoch time: 437.43 s
2025-08-05 10:11:02.835864: train_loss 3.6096
2025-08-05 10:11:02.841037: train_loss 3.6096
2025-08-05 10:11:02.862375: Epoch time: 434.92 s
2025-08-05 10:11:02.861260: Epoch time: 435.06 s
2025-08-05 10:11:06.958033: 
2025-08-05 10:11:06.960732: Epoch 182
2025-08-05 10:11:06.962939: Current learning rate: 0.00028
2025-08-05 10:11:08.940841: 
2025-08-05 10:11:08.943002: Epoch 182
2025-08-05 10:11:08.945737: Current learning rate: 0.00028
2025-08-05 10:11:11.217470: 
2025-08-05 10:11:11.217898: Epoch 182
2025-08-05 10:11:11.218523: Current learning rate: 0.00028
2025-08-05 10:18:28.969979: train_loss 3.608
2025-08-05 10:18:28.972353: Epoch time: 442.01 s
2025-08-05 10:18:28.971871: train_loss 3.608
2025-08-05 10:18:28.974286: Epoch time: 440.03 s
2025-08-05 10:18:28.977904: train_loss 3.608
2025-08-05 10:18:28.979198: Epoch time: 437.76 s
2025-08-05 10:18:33.387713: 
2025-08-05 10:18:33.388228: Epoch 183
2025-08-05 10:18:33.392014: Current learning rate: 0.00028
2025-08-05 10:18:34.831652: 
2025-08-05 10:18:34.832003: Epoch 183
2025-08-05 10:18:34.832553: Current learning rate: 0.00028
2025-08-05 10:18:34.952757: 
2025-08-05 10:18:34.955042: Epoch 183
2025-08-05 10:18:34.955661: Current learning rate: 0.00028
2025-08-05 10:26:00.136636: train_loss 3.6101
2025-08-05 10:26:00.137404: train_loss 3.6101
2025-08-05 10:26:00.143703: Epoch time: 445.31 s
2025-08-05 10:26:00.139794: train_loss 3.6101
2025-08-05 10:26:00.142999: Epoch time: 445.18 s
2025-08-05 10:26:00.145709: Epoch time: 446.75 s
2025-08-05 10:26:04.302523: 
2025-08-05 10:26:04.303655: Epoch 184
2025-08-05 10:26:04.305512: Current learning rate: 0.00028
2025-08-05 10:26:05.654584: 
2025-08-05 10:26:05.677735: Epoch 184
2025-08-05 10:26:05.697997: Current learning rate: 0.00028
2025-08-05 10:26:06.165599: 
2025-08-05 10:26:06.167769: Epoch 184
2025-08-05 10:26:06.178965: Current learning rate: 0.00028

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Tue Aug  5 10:27:23 2025
Driver Version                            : 570.133.20
CUDA Version                              : 12.8

Attached GPUs                             : 4
GPU 00000000:4E:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224049
            GPU Utilization               : 94 %
            Memory Utilization            : 33 %
            Max memory usage              : 79620 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:5F:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224050
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80262 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:CB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224051
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80274 MiB
            Time                          : 0 ms
            Is Running                    : 1

GPU 00000000:DB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224052
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 79648 MiB
            Time                          : 0 ms
            Is Running                    : 1

Tue Aug  5 10:27:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:4E:00.0 Off |                    0 |
| N/A   62C    P0            322W /  700W |   79639MiB /  81559MiB |     51%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:5F:00.0 Off |                    0 |
| N/A   60C    P0            323W /  700W |   80281MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   59C    P0            319W /  700W |   80293MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   61C    P0            314W /  700W |   79667MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

======== GPU REPORT ========

======== GPU REPORT ========

======== GPU REPORT ========

==============NVSMI LOG==============

Timestamp                                 : Tue Aug  5 10:27:27 2025
Driver Version                            : 570.133.20
CUDA Version                              : 12.8

Attached GPUs                             : 4
GPU 00000000:4E:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2027993
            GPU Utilization               : 95 %
            Memory Utilization            : 33 %
            Max memory usage              : 79624 MiB
            Time                          : 82590533 ms
            Is Running                    : 0

GPU 00000000:5F:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2027994
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 79612 MiB
            Time                          : 82592006 ms
            Is Running                    : 0

GPU 00000000:CB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2027995
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 79588 MiB
            Time                          : 82591097 ms
            Is Running                    : 0

GPU 00000000:DB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2027996
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 79580 MiB
            Time                          : 82591437 ms
            Is Running                    : 0


==============NVSMI LOG==============

Timestamp                                 : Tue Aug  5 10:27:27 2025
Driver Version                            : 570.133.20
CUDA Version                              : 12.8

Attached GPUs                             : 4
GPU 00000000:4E:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224049
            GPU Utilization               : 94 %
            Memory Utilization            : 33 %
            Max memory usage              : 79620 MiB
            Time                          : 82591016 ms
            Is Running                    : 0

GPU 00000000:5F:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224050
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80262 MiB
            Time                          : 82592148 ms
            Is Running                    : 0

GPU 00000000:CB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224051
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80274 MiB
            Time                          : 82591324 ms
            Is Running                    : 0

GPU 00000000:DB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 2224052
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 79648 MiB
            Time                          : 82590413 ms
            Is Running                    : 0


==============NVSMI LOG==============

Timestamp                                 : Tue Aug  5 10:27:27 2025
Driver Version                            : 570.133.20
CUDA Version                              : 12.8

Attached GPUs                             : 4
GPU 00000000:4E:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1473252
            GPU Utilization               : 95 %
            Memory Utilization            : 33 %
            Max memory usage              : 79616 MiB
            Time                          : 82592081 ms
            Is Running                    : 0

GPU 00000000:5F:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1473253
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80266 MiB
            Time                          : 82591215 ms
            Is Running                    : 0

GPU 00000000:CB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1473254
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80274 MiB
            Time                          : 82591912 ms
            Is Running                    : 0

GPU 00000000:DB:00.0
    Accounting Mode                       : Enabled
    Accounting Mode Buffer Size           : 4000
    Accounted Processes
        Process ID                        : 1473255
            GPU Utilization               : 96 %
            Memory Utilization            : 33 %
            Max memory usage              : 80364 MiB
            Time                          : 82591463 ms
            Is Running                    : 0

Tue Aug  5 10:27:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:4E:00.0 Off |                    0 |
| N/A   52C    P0            112W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:5F:00.0 Off |                    0 |
| N/A   52C    P0            133W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   53C    P0            135W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   54C    P0            133W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Tue Aug  5 10:27:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
Tue Aug  5 10:27:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:4E:00.0 Off |                    0 |
| N/A   55C    P0            138W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:4E:00.0 Off |                    0 |
| N/A   54C    P0            130W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:5F:00.0 Off |                    0 |
| N/A   53C    P0            135W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:5F:00.0 Off |                    0 |
| N/A   53C    P0            143W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   53C    P0            139W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   55C    P0            142W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   54C    P0            140W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   54C    P0             97W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
